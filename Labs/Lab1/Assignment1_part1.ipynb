{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of assignment1_part1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dareenhussein/PMDL/blob/main/Copy_of_assignment1_part1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8HNdarz1uIIZ"
      },
      "source": [
        "<table align=\"center\">\n",
        "  <td align=\"center\"><a target=\"_blank\" href=\"https://colab.research.google.com/github/KhaledElTahan/DeepLearning/blob/master/Labs/lab1/lab1_part1.ipynb\">\n",
        "        <img src=\"http://introtodeeplearning.com/images/colab/colab.png?v2.0\"  style=\"padding-bottom:5px;\" />Run in Google Colab</a></td>\n",
        "</table>\n",
        "\n",
        "# Copyright Information"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iSA7aTOivRhA"
      },
      "source": [
        "**Parts of this lab are based on Kaggle kernels.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WfATj8pltlxh"
      },
      "source": [
        "# Lab 1 - Part1: Linear Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cOy1mYdKw5TE"
      },
      "source": [
        "![Linear Regression](https://raw.githubusercontent.com/KhaledElTahan/AUC-DeepLearning/master/Labs/lab1/linear_regression.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MsLKJ-4kwLYv"
      },
      "source": [
        "## 1.1.1 Problem Statement"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RQxlb182u_gv"
      },
      "source": [
        "The problem we are trying to solve here is finding a new house which is suitable to our needs and the budget we assigned. The client who wants to buy the new house did her research and found some houses. She wrote the details of each house she visited including location, sale condition, sale type, house price, among others. She needs some help to know how much she is expected to pay to get a house that conforms with her specific needs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GUidwfyPvcfv"
      },
      "source": [
        "Your task is to build a linear regression model that helps her to predict the house price depending on the given attributes she collected."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PeXilc2Nvg-g"
      },
      "source": [
        "## 1.1.2 Problem Details"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bsHAbLgL1MsY"
      },
      "source": [
        "Let's dive into the code, explain it and show you the parts you need to fill!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7n5JS__dwfiH"
      },
      "source": [
        "### 1.1.2.1 Import Needed packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xm9MPDVSsDpu"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib\n",
        "import tensorflow as tf\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.stats import skew\n",
        "from scipy.stats.stats import pearsonr\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.regularizers import l1, l2\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7EfciecZwm6L"
      },
      "source": [
        "### 1.1.2.2 Configure Notebook"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EW1TyciwsN9_"
      },
      "source": [
        "%config InlineBackend.figure_format = 'retina' #set 'png' here when working on notebook\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a6PJSAiyx4HM"
      },
      "source": [
        "### 1.1.2.3 Work on the Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wcVsD6V1x-Ty"
      },
      "source": [
        "This dataset contains 80 features that demonstrate the state of the house and our target which is the house price.\n",
        "\n",
        "We begin by loading the train and test splits of the dataset using pandas."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P6zYNeXVvvRA"
      },
      "source": [
        "train = pd.read_csv(\"https://raw.githubusercontent.com/KhaledElTahan/DeepLearning/master/Labs/lab1/lab1_housing_train.csv\")\n",
        "test = pd.read_csv(\"https://raw.githubusercontent.com/KhaledElTahan/DeepLearning/master/Labs/lab1/lab1_housing_test.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zyl1lDq-yVFG"
      },
      "source": [
        "You can have a look at the train split of the dataset using the head command. I very much encourage you to have a deeper look on the dataset file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b2JWObzgyY9t",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 253
        },
        "outputId": "030505d9-df0b-4ddc-87e8-bb942c058051"
      },
      "source": [
        "train.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Id</th>\n",
              "      <th>MSSubClass</th>\n",
              "      <th>MSZoning</th>\n",
              "      <th>LotFrontage</th>\n",
              "      <th>LotArea</th>\n",
              "      <th>Street</th>\n",
              "      <th>Alley</th>\n",
              "      <th>LotShape</th>\n",
              "      <th>LandContour</th>\n",
              "      <th>Utilities</th>\n",
              "      <th>LotConfig</th>\n",
              "      <th>LandSlope</th>\n",
              "      <th>Neighborhood</th>\n",
              "      <th>Condition1</th>\n",
              "      <th>Condition2</th>\n",
              "      <th>BldgType</th>\n",
              "      <th>HouseStyle</th>\n",
              "      <th>OverallQual</th>\n",
              "      <th>OverallCond</th>\n",
              "      <th>YearBuilt</th>\n",
              "      <th>YearRemodAdd</th>\n",
              "      <th>RoofStyle</th>\n",
              "      <th>RoofMatl</th>\n",
              "      <th>Exterior1st</th>\n",
              "      <th>Exterior2nd</th>\n",
              "      <th>MasVnrType</th>\n",
              "      <th>MasVnrArea</th>\n",
              "      <th>ExterQual</th>\n",
              "      <th>ExterCond</th>\n",
              "      <th>Foundation</th>\n",
              "      <th>BsmtQual</th>\n",
              "      <th>BsmtCond</th>\n",
              "      <th>BsmtExposure</th>\n",
              "      <th>BsmtFinType1</th>\n",
              "      <th>BsmtFinSF1</th>\n",
              "      <th>BsmtFinType2</th>\n",
              "      <th>BsmtFinSF2</th>\n",
              "      <th>BsmtUnfSF</th>\n",
              "      <th>TotalBsmtSF</th>\n",
              "      <th>Heating</th>\n",
              "      <th>...</th>\n",
              "      <th>CentralAir</th>\n",
              "      <th>Electrical</th>\n",
              "      <th>1stFlrSF</th>\n",
              "      <th>2ndFlrSF</th>\n",
              "      <th>LowQualFinSF</th>\n",
              "      <th>GrLivArea</th>\n",
              "      <th>BsmtFullBath</th>\n",
              "      <th>BsmtHalfBath</th>\n",
              "      <th>FullBath</th>\n",
              "      <th>HalfBath</th>\n",
              "      <th>BedroomAbvGr</th>\n",
              "      <th>KitchenAbvGr</th>\n",
              "      <th>KitchenQual</th>\n",
              "      <th>TotRmsAbvGrd</th>\n",
              "      <th>Functional</th>\n",
              "      <th>Fireplaces</th>\n",
              "      <th>FireplaceQu</th>\n",
              "      <th>GarageType</th>\n",
              "      <th>GarageYrBlt</th>\n",
              "      <th>GarageFinish</th>\n",
              "      <th>GarageCars</th>\n",
              "      <th>GarageArea</th>\n",
              "      <th>GarageQual</th>\n",
              "      <th>GarageCond</th>\n",
              "      <th>PavedDrive</th>\n",
              "      <th>WoodDeckSF</th>\n",
              "      <th>OpenPorchSF</th>\n",
              "      <th>EnclosedPorch</th>\n",
              "      <th>3SsnPorch</th>\n",
              "      <th>ScreenPorch</th>\n",
              "      <th>PoolArea</th>\n",
              "      <th>PoolQC</th>\n",
              "      <th>Fence</th>\n",
              "      <th>MiscFeature</th>\n",
              "      <th>MiscVal</th>\n",
              "      <th>MoSold</th>\n",
              "      <th>YrSold</th>\n",
              "      <th>SaleType</th>\n",
              "      <th>SaleCondition</th>\n",
              "      <th>SalePrice</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>60</td>\n",
              "      <td>RL</td>\n",
              "      <td>65.0</td>\n",
              "      <td>8450</td>\n",
              "      <td>Pave</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Reg</td>\n",
              "      <td>Lvl</td>\n",
              "      <td>AllPub</td>\n",
              "      <td>Inside</td>\n",
              "      <td>Gtl</td>\n",
              "      <td>CollgCr</td>\n",
              "      <td>Norm</td>\n",
              "      <td>Norm</td>\n",
              "      <td>1Fam</td>\n",
              "      <td>2Story</td>\n",
              "      <td>7</td>\n",
              "      <td>5</td>\n",
              "      <td>2003</td>\n",
              "      <td>2003</td>\n",
              "      <td>Gable</td>\n",
              "      <td>CompShg</td>\n",
              "      <td>VinylSd</td>\n",
              "      <td>VinylSd</td>\n",
              "      <td>BrkFace</td>\n",
              "      <td>196.0</td>\n",
              "      <td>Gd</td>\n",
              "      <td>TA</td>\n",
              "      <td>PConc</td>\n",
              "      <td>Gd</td>\n",
              "      <td>TA</td>\n",
              "      <td>No</td>\n",
              "      <td>GLQ</td>\n",
              "      <td>706</td>\n",
              "      <td>Unf</td>\n",
              "      <td>0</td>\n",
              "      <td>150</td>\n",
              "      <td>856</td>\n",
              "      <td>GasA</td>\n",
              "      <td>...</td>\n",
              "      <td>Y</td>\n",
              "      <td>SBrkr</td>\n",
              "      <td>856</td>\n",
              "      <td>854</td>\n",
              "      <td>0</td>\n",
              "      <td>1710</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>Gd</td>\n",
              "      <td>8</td>\n",
              "      <td>Typ</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Attchd</td>\n",
              "      <td>2003.0</td>\n",
              "      <td>RFn</td>\n",
              "      <td>2</td>\n",
              "      <td>548</td>\n",
              "      <td>TA</td>\n",
              "      <td>TA</td>\n",
              "      <td>Y</td>\n",
              "      <td>0</td>\n",
              "      <td>61</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>2008</td>\n",
              "      <td>WD</td>\n",
              "      <td>Normal</td>\n",
              "      <td>208500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>20</td>\n",
              "      <td>RL</td>\n",
              "      <td>80.0</td>\n",
              "      <td>9600</td>\n",
              "      <td>Pave</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Reg</td>\n",
              "      <td>Lvl</td>\n",
              "      <td>AllPub</td>\n",
              "      <td>FR2</td>\n",
              "      <td>Gtl</td>\n",
              "      <td>Veenker</td>\n",
              "      <td>Feedr</td>\n",
              "      <td>Norm</td>\n",
              "      <td>1Fam</td>\n",
              "      <td>1Story</td>\n",
              "      <td>6</td>\n",
              "      <td>8</td>\n",
              "      <td>1976</td>\n",
              "      <td>1976</td>\n",
              "      <td>Gable</td>\n",
              "      <td>CompShg</td>\n",
              "      <td>MetalSd</td>\n",
              "      <td>MetalSd</td>\n",
              "      <td>None</td>\n",
              "      <td>0.0</td>\n",
              "      <td>TA</td>\n",
              "      <td>TA</td>\n",
              "      <td>CBlock</td>\n",
              "      <td>Gd</td>\n",
              "      <td>TA</td>\n",
              "      <td>Gd</td>\n",
              "      <td>ALQ</td>\n",
              "      <td>978</td>\n",
              "      <td>Unf</td>\n",
              "      <td>0</td>\n",
              "      <td>284</td>\n",
              "      <td>1262</td>\n",
              "      <td>GasA</td>\n",
              "      <td>...</td>\n",
              "      <td>Y</td>\n",
              "      <td>SBrkr</td>\n",
              "      <td>1262</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1262</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>TA</td>\n",
              "      <td>6</td>\n",
              "      <td>Typ</td>\n",
              "      <td>1</td>\n",
              "      <td>TA</td>\n",
              "      <td>Attchd</td>\n",
              "      <td>1976.0</td>\n",
              "      <td>RFn</td>\n",
              "      <td>2</td>\n",
              "      <td>460</td>\n",
              "      <td>TA</td>\n",
              "      <td>TA</td>\n",
              "      <td>Y</td>\n",
              "      <td>298</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>2007</td>\n",
              "      <td>WD</td>\n",
              "      <td>Normal</td>\n",
              "      <td>181500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>60</td>\n",
              "      <td>RL</td>\n",
              "      <td>68.0</td>\n",
              "      <td>11250</td>\n",
              "      <td>Pave</td>\n",
              "      <td>NaN</td>\n",
              "      <td>IR1</td>\n",
              "      <td>Lvl</td>\n",
              "      <td>AllPub</td>\n",
              "      <td>Inside</td>\n",
              "      <td>Gtl</td>\n",
              "      <td>CollgCr</td>\n",
              "      <td>Norm</td>\n",
              "      <td>Norm</td>\n",
              "      <td>1Fam</td>\n",
              "      <td>2Story</td>\n",
              "      <td>7</td>\n",
              "      <td>5</td>\n",
              "      <td>2001</td>\n",
              "      <td>2002</td>\n",
              "      <td>Gable</td>\n",
              "      <td>CompShg</td>\n",
              "      <td>VinylSd</td>\n",
              "      <td>VinylSd</td>\n",
              "      <td>BrkFace</td>\n",
              "      <td>162.0</td>\n",
              "      <td>Gd</td>\n",
              "      <td>TA</td>\n",
              "      <td>PConc</td>\n",
              "      <td>Gd</td>\n",
              "      <td>TA</td>\n",
              "      <td>Mn</td>\n",
              "      <td>GLQ</td>\n",
              "      <td>486</td>\n",
              "      <td>Unf</td>\n",
              "      <td>0</td>\n",
              "      <td>434</td>\n",
              "      <td>920</td>\n",
              "      <td>GasA</td>\n",
              "      <td>...</td>\n",
              "      <td>Y</td>\n",
              "      <td>SBrkr</td>\n",
              "      <td>920</td>\n",
              "      <td>866</td>\n",
              "      <td>0</td>\n",
              "      <td>1786</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>Gd</td>\n",
              "      <td>6</td>\n",
              "      <td>Typ</td>\n",
              "      <td>1</td>\n",
              "      <td>TA</td>\n",
              "      <td>Attchd</td>\n",
              "      <td>2001.0</td>\n",
              "      <td>RFn</td>\n",
              "      <td>2</td>\n",
              "      <td>608</td>\n",
              "      <td>TA</td>\n",
              "      <td>TA</td>\n",
              "      <td>Y</td>\n",
              "      <td>0</td>\n",
              "      <td>42</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>9</td>\n",
              "      <td>2008</td>\n",
              "      <td>WD</td>\n",
              "      <td>Normal</td>\n",
              "      <td>223500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>70</td>\n",
              "      <td>RL</td>\n",
              "      <td>60.0</td>\n",
              "      <td>9550</td>\n",
              "      <td>Pave</td>\n",
              "      <td>NaN</td>\n",
              "      <td>IR1</td>\n",
              "      <td>Lvl</td>\n",
              "      <td>AllPub</td>\n",
              "      <td>Corner</td>\n",
              "      <td>Gtl</td>\n",
              "      <td>Crawfor</td>\n",
              "      <td>Norm</td>\n",
              "      <td>Norm</td>\n",
              "      <td>1Fam</td>\n",
              "      <td>2Story</td>\n",
              "      <td>7</td>\n",
              "      <td>5</td>\n",
              "      <td>1915</td>\n",
              "      <td>1970</td>\n",
              "      <td>Gable</td>\n",
              "      <td>CompShg</td>\n",
              "      <td>Wd Sdng</td>\n",
              "      <td>Wd Shng</td>\n",
              "      <td>None</td>\n",
              "      <td>0.0</td>\n",
              "      <td>TA</td>\n",
              "      <td>TA</td>\n",
              "      <td>BrkTil</td>\n",
              "      <td>TA</td>\n",
              "      <td>Gd</td>\n",
              "      <td>No</td>\n",
              "      <td>ALQ</td>\n",
              "      <td>216</td>\n",
              "      <td>Unf</td>\n",
              "      <td>0</td>\n",
              "      <td>540</td>\n",
              "      <td>756</td>\n",
              "      <td>GasA</td>\n",
              "      <td>...</td>\n",
              "      <td>Y</td>\n",
              "      <td>SBrkr</td>\n",
              "      <td>961</td>\n",
              "      <td>756</td>\n",
              "      <td>0</td>\n",
              "      <td>1717</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>Gd</td>\n",
              "      <td>7</td>\n",
              "      <td>Typ</td>\n",
              "      <td>1</td>\n",
              "      <td>Gd</td>\n",
              "      <td>Detchd</td>\n",
              "      <td>1998.0</td>\n",
              "      <td>Unf</td>\n",
              "      <td>3</td>\n",
              "      <td>642</td>\n",
              "      <td>TA</td>\n",
              "      <td>TA</td>\n",
              "      <td>Y</td>\n",
              "      <td>0</td>\n",
              "      <td>35</td>\n",
              "      <td>272</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>2006</td>\n",
              "      <td>WD</td>\n",
              "      <td>Abnorml</td>\n",
              "      <td>140000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>60</td>\n",
              "      <td>RL</td>\n",
              "      <td>84.0</td>\n",
              "      <td>14260</td>\n",
              "      <td>Pave</td>\n",
              "      <td>NaN</td>\n",
              "      <td>IR1</td>\n",
              "      <td>Lvl</td>\n",
              "      <td>AllPub</td>\n",
              "      <td>FR2</td>\n",
              "      <td>Gtl</td>\n",
              "      <td>NoRidge</td>\n",
              "      <td>Norm</td>\n",
              "      <td>Norm</td>\n",
              "      <td>1Fam</td>\n",
              "      <td>2Story</td>\n",
              "      <td>8</td>\n",
              "      <td>5</td>\n",
              "      <td>2000</td>\n",
              "      <td>2000</td>\n",
              "      <td>Gable</td>\n",
              "      <td>CompShg</td>\n",
              "      <td>VinylSd</td>\n",
              "      <td>VinylSd</td>\n",
              "      <td>BrkFace</td>\n",
              "      <td>350.0</td>\n",
              "      <td>Gd</td>\n",
              "      <td>TA</td>\n",
              "      <td>PConc</td>\n",
              "      <td>Gd</td>\n",
              "      <td>TA</td>\n",
              "      <td>Av</td>\n",
              "      <td>GLQ</td>\n",
              "      <td>655</td>\n",
              "      <td>Unf</td>\n",
              "      <td>0</td>\n",
              "      <td>490</td>\n",
              "      <td>1145</td>\n",
              "      <td>GasA</td>\n",
              "      <td>...</td>\n",
              "      <td>Y</td>\n",
              "      <td>SBrkr</td>\n",
              "      <td>1145</td>\n",
              "      <td>1053</td>\n",
              "      <td>0</td>\n",
              "      <td>2198</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>Gd</td>\n",
              "      <td>9</td>\n",
              "      <td>Typ</td>\n",
              "      <td>1</td>\n",
              "      <td>TA</td>\n",
              "      <td>Attchd</td>\n",
              "      <td>2000.0</td>\n",
              "      <td>RFn</td>\n",
              "      <td>3</td>\n",
              "      <td>836</td>\n",
              "      <td>TA</td>\n",
              "      <td>TA</td>\n",
              "      <td>Y</td>\n",
              "      <td>192</td>\n",
              "      <td>84</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>12</td>\n",
              "      <td>2008</td>\n",
              "      <td>WD</td>\n",
              "      <td>Normal</td>\n",
              "      <td>250000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 81 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   Id  MSSubClass MSZoning  ...  SaleType  SaleCondition SalePrice\n",
              "0   1          60       RL  ...        WD         Normal    208500\n",
              "1   2          20       RL  ...        WD         Normal    181500\n",
              "2   3          60       RL  ...        WD         Normal    223500\n",
              "3   4          70       RL  ...        WD        Abnorml    140000\n",
              "4   5          60       RL  ...        WD         Normal    250000\n",
              "\n",
              "[5 rows x 81 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 241
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VrXVoqAU0h1g"
      },
      "source": [
        "Data preprocessing:\n",
        "* First I'll transform the skewed numeric features by taking log(feature + 1) - this will make the features more normal\n",
        "* Create Dummy variables for the categorical features\n",
        "* Replace the numeric missing values (NaN's) with the mean of their respective columns\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nCCi1svksOj4"
      },
      "source": [
        "# Concatenate all the data\n",
        "# We do this to be able to preprocess on the whole dataset\n",
        "all_data = pd.concat((train.loc[:,'MSSubClass':'SaleCondition'],\n",
        "                      test.loc[:,'MSSubClass':'SaleCondition']))\n",
        "\n",
        "# Log transform the target y in training data - by reference inside all\n",
        "train[\"SalePrice\"] = np.log1p(train[\"SalePrice\"])\n",
        "\n",
        "# Log transform skewed numeric features:\n",
        "\n",
        "# Get Numerical Fields\n",
        "numeric_feats = all_data.dtypes[all_data.dtypes != \"object\"].index \n",
        "\n",
        "skewed_feats = train[numeric_feats].apply(lambda x: skew(x.dropna())) #compute skewnessc\n",
        "skewed_feats = skewed_feats[skewed_feats > 0.75] # Get Skewed Columns\n",
        "skewed_feats = skewed_feats.index # Get Skewed Columns indices\n",
        "\n",
        "# Log scale skewed columns\n",
        "# Normalize the skewed distribution for better regression\n",
        "all_data[skewed_feats] = np.log1p(all_data[skewed_feats])\n",
        "\n",
        "# Create Dummy variables for the categorical features \n",
        "all_data = pd.get_dummies(all_data) \n",
        "\n",
        "# Replace the numeric missing values (NaN's) with the mean of their respective columns\n",
        "all_data = all_data.fillna(all_data.mean())\n",
        "\n",
        "# Split the data to training & testing\n",
        "X_train = all_data[:train.shape[0]]\n",
        "X_test = all_data[train.shape[0]:]\n",
        "y = train.SalePrice\n",
        "\n",
        "# Standardize features by removing the mean and scaling to unit variance\n",
        "# z = (x - u) / s\n",
        "X_train = StandardScaler().fit_transform(X_train)\n",
        "\n",
        "#split training data into training & validation, default splitting is 25% validation\n",
        "X_tr, X_val, y_tr, y_val = train_test_split(X_train, y, random_state = 3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SVa9K1Ga1vlo"
      },
      "source": [
        "### 1.1.2.4 Define your model here (TODO)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IocosMrh2W3X"
      },
      "source": [
        "One important note you need to be aware of, linear regression is a neural network with only one perceptron (i.e. dense layer with one node) with a linear activation (i.e. no activation function). \n",
        "\n",
        "![One Perceptron Neural Network](https://raw.githubusercontent.com/KhaledElTahan/AUC-DeepLearning/master/Labs/lab1/perceptron.png)\n",
        "\n",
        "Use this note to define a **sequential model of one dense layer with one unit using Tensorflow.Keras**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oh1NvkNXsVVG"
      },
      "source": [
        "# TODO: Define the Model using Tensorflow.Keras\n",
        "# adding regularization to modify the model since it is overfitting the data\n",
        "model = Sequential([tf.keras.layers.Dense(1,input_shape=(X_tr.shape[1],), activation = None, \n",
        "                                          kernel_regularizer=l1(1.3))])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BCBQmsxtKRTw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aae057f7-0c74-48d4-94e6-cf460e0f0d37"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jSa178n-2BM-"
      },
      "source": [
        "### 1.1.2.5 Compile your model and print a summary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fg6Yy5oDsXyK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9dae06c7-8d44-4a29-89c2-e04861d9bd0d"
      },
      "source": [
        "model.compile(loss = \"mean_squared_error\", optimizer = \"adam\")\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_28\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_28 (Dense)             (None, 1)                 289       \n",
            "=================================================================\n",
            "Total params: 289\n",
            "Trainable params: 289\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9FgD-Kqo3fwb"
      },
      "source": [
        "### 1.1.2.6 Train your model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aA4pLcN73waK"
      },
      "source": [
        "Fit your model into the training data, use the validation data to be able to plot the loss decrement during the training. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cAkW1KTm3TGU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "99d77bee-3d60-4a70-a954-258b2ac1e241"
      },
      "source": [
        "hist = model.fit(X_tr, y_tr, validation_data = (X_val, y_val), epochs = 500)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/500\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 173.6327 - val_loss: 167.4746\n",
            "Epoch 2/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 168.5718 - val_loss: 163.3028\n",
            "Epoch 3/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 162.7796 - val_loss: 159.7663\n",
            "Epoch 4/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 157.8426 - val_loss: 156.4170\n",
            "Epoch 5/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 153.0389 - val_loss: 153.2455\n",
            "Epoch 6/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 149.7723 - val_loss: 150.3077\n",
            "Epoch 7/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 146.5666 - val_loss: 147.5352\n",
            "Epoch 8/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 143.8301 - val_loss: 145.1759\n",
            "Epoch 9/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 141.8705 - val_loss: 142.9023\n",
            "Epoch 10/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 139.0007 - val_loss: 140.9729\n",
            "Epoch 11/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 137.7779 - val_loss: 139.0855\n",
            "Epoch 12/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 137.0839 - val_loss: 137.4094\n",
            "Epoch 13/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 136.0554 - val_loss: 135.9367\n",
            "Epoch 14/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 134.8401 - val_loss: 134.8347\n",
            "Epoch 15/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 133.8057 - val_loss: 133.7477\n",
            "Epoch 16/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 133.0055 - val_loss: 132.8037\n",
            "Epoch 17/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 132.3109 - val_loss: 131.8322\n",
            "Epoch 18/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 131.2564 - val_loss: 130.8714\n",
            "Epoch 19/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 130.0367 - val_loss: 130.0149\n",
            "Epoch 20/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 129.9117 - val_loss: 129.2035\n",
            "Epoch 21/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 129.0540 - val_loss: 128.3860\n",
            "Epoch 22/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 127.8402 - val_loss: 127.6875\n",
            "Epoch 23/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 127.4739 - val_loss: 126.8768\n",
            "Epoch 24/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 126.8370 - val_loss: 126.0683\n",
            "Epoch 25/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 125.9141 - val_loss: 125.3552\n",
            "Epoch 26/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 125.1237 - val_loss: 124.5137\n",
            "Epoch 27/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 124.6607 - val_loss: 123.7940\n",
            "Epoch 28/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 123.6209 - val_loss: 123.0556\n",
            "Epoch 29/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 122.6913 - val_loss: 122.2811\n",
            "Epoch 30/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 121.8948 - val_loss: 121.5711\n",
            "Epoch 31/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 121.6797 - val_loss: 120.8146\n",
            "Epoch 32/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 120.0986 - val_loss: 120.0372\n",
            "Epoch 33/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 119.3272 - val_loss: 119.3596\n",
            "Epoch 34/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 119.0131 - val_loss: 118.6090\n",
            "Epoch 35/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 118.5988 - val_loss: 117.8360\n",
            "Epoch 36/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 117.6911 - val_loss: 117.1815\n",
            "Epoch 37/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 117.0479 - val_loss: 116.2954\n",
            "Epoch 38/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 115.9985 - val_loss: 115.6343\n",
            "Epoch 39/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 115.7139 - val_loss: 114.9431\n",
            "Epoch 40/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 114.9308 - val_loss: 114.1633\n",
            "Epoch 41/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 114.2214 - val_loss: 113.5096\n",
            "Epoch 42/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 113.3813 - val_loss: 112.8331\n",
            "Epoch 43/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 112.9327 - val_loss: 112.1578\n",
            "Epoch 44/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 111.6233 - val_loss: 111.2662\n",
            "Epoch 45/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 111.5348 - val_loss: 110.6667\n",
            "Epoch 46/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 110.6052 - val_loss: 109.9743\n",
            "Epoch 47/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 110.0110 - val_loss: 109.2732\n",
            "Epoch 48/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 109.1002 - val_loss: 108.6229\n",
            "Epoch 49/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 108.5858 - val_loss: 107.7791\n",
            "Epoch 50/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 107.8138 - val_loss: 107.1702\n",
            "Epoch 51/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 106.9466 - val_loss: 106.5502\n",
            "Epoch 52/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 106.3728 - val_loss: 105.8532\n",
            "Epoch 53/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 105.7034 - val_loss: 105.1298\n",
            "Epoch 54/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 105.3732 - val_loss: 104.5727\n",
            "Epoch 55/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 104.2134 - val_loss: 103.8180\n",
            "Epoch 56/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 103.9333 - val_loss: 103.1832\n",
            "Epoch 57/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 103.4213 - val_loss: 102.4906\n",
            "Epoch 58/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 102.2163 - val_loss: 101.8639\n",
            "Epoch 59/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 101.6189 - val_loss: 101.1644\n",
            "Epoch 60/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 100.7088 - val_loss: 100.4629\n",
            "Epoch 61/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 100.5987 - val_loss: 99.8406\n",
            "Epoch 62/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 99.6671 - val_loss: 99.1465\n",
            "Epoch 63/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 98.9828 - val_loss: 98.5448\n",
            "Epoch 64/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 98.3790 - val_loss: 97.8727\n",
            "Epoch 65/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 97.9759 - val_loss: 97.2557\n",
            "Epoch 66/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 97.0284 - val_loss: 96.6166\n",
            "Epoch 67/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 96.7017 - val_loss: 95.8813\n",
            "Epoch 68/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 95.6777 - val_loss: 95.2856\n",
            "Epoch 69/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 95.0554 - val_loss: 94.6487\n",
            "Epoch 70/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 94.4032 - val_loss: 94.1443\n",
            "Epoch 71/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 94.1397 - val_loss: 93.4777\n",
            "Epoch 72/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 93.3555 - val_loss: 92.7657\n",
            "Epoch 73/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 92.7958 - val_loss: 92.1529\n",
            "Epoch 74/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 92.0036 - val_loss: 91.5235\n",
            "Epoch 75/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 91.2440 - val_loss: 90.8280\n",
            "Epoch 76/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 90.6764 - val_loss: 90.3305\n",
            "Epoch 77/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 90.1822 - val_loss: 89.6935\n",
            "Epoch 78/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 89.4924 - val_loss: 89.0717\n",
            "Epoch 79/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 88.9953 - val_loss: 88.5164\n",
            "Epoch 80/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 88.6690 - val_loss: 87.8746\n",
            "Epoch 81/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 87.9691 - val_loss: 87.2853\n",
            "Epoch 82/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 87.0584 - val_loss: 86.6735\n",
            "Epoch 83/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 86.3715 - val_loss: 86.0235\n",
            "Epoch 84/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 86.1848 - val_loss: 85.4502\n",
            "Epoch 85/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 85.3308 - val_loss: 84.8571\n",
            "Epoch 86/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 84.7598 - val_loss: 84.2210\n",
            "Epoch 87/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 84.3452 - val_loss: 83.6705\n",
            "Epoch 88/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 83.2362 - val_loss: 83.1981\n",
            "Epoch 89/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 83.2122 - val_loss: 82.5272\n",
            "Epoch 90/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 82.2067 - val_loss: 81.9254\n",
            "Epoch 91/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 81.9592 - val_loss: 81.3564\n",
            "Epoch 92/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 81.1672 - val_loss: 80.7743\n",
            "Epoch 93/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 80.5176 - val_loss: 80.2214\n",
            "Epoch 94/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 80.4788 - val_loss: 79.6571\n",
            "Epoch 95/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 79.6950 - val_loss: 79.0891\n",
            "Epoch 96/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 79.1102 - val_loss: 78.5544\n",
            "Epoch 97/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 77.9436 - val_loss: 77.9835\n",
            "Epoch 98/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 77.8402 - val_loss: 77.4203\n",
            "Epoch 99/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 77.4236 - val_loss: 76.7555\n",
            "Epoch 100/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 76.3314 - val_loss: 76.2121\n",
            "Epoch 101/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 76.4637 - val_loss: 75.6638\n",
            "Epoch 102/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 75.6853 - val_loss: 75.1475\n",
            "Epoch 103/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 75.0432 - val_loss: 74.6378\n",
            "Epoch 104/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 74.3992 - val_loss: 74.0301\n",
            "Epoch 105/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 73.7760 - val_loss: 73.4381\n",
            "Epoch 106/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 73.4949 - val_loss: 72.8979\n",
            "Epoch 107/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 72.8630 - val_loss: 72.3557\n",
            "Epoch 108/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 72.5189 - val_loss: 71.8673\n",
            "Epoch 109/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 71.8766 - val_loss: 71.4164\n",
            "Epoch 110/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 71.2379 - val_loss: 70.8214\n",
            "Epoch 111/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 70.6494 - val_loss: 70.2436\n",
            "Epoch 112/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 70.0734 - val_loss: 69.6991\n",
            "Epoch 113/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 69.7646 - val_loss: 69.1604\n",
            "Epoch 114/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 69.3467 - val_loss: 68.6807\n",
            "Epoch 115/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 68.2953 - val_loss: 68.1314\n",
            "Epoch 116/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 67.6681 - val_loss: 67.6782\n",
            "Epoch 117/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 67.5752 - val_loss: 67.1000\n",
            "Epoch 118/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 66.9738 - val_loss: 66.5754\n",
            "Epoch 119/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 66.6349 - val_loss: 65.9868\n",
            "Epoch 120/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 66.2265 - val_loss: 65.5885\n",
            "Epoch 121/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 65.8752 - val_loss: 65.0765\n",
            "Epoch 122/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 65.2455 - val_loss: 64.6147\n",
            "Epoch 123/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 64.5970 - val_loss: 64.0792\n",
            "Epoch 124/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 64.2191 - val_loss: 63.5494\n",
            "Epoch 125/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 63.7300 - val_loss: 63.0386\n",
            "Epoch 126/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 62.7380 - val_loss: 62.5520\n",
            "Epoch 127/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 62.5025 - val_loss: 62.0379\n",
            "Epoch 128/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 62.4990 - val_loss: 61.5260\n",
            "Epoch 129/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 61.6397 - val_loss: 61.0631\n",
            "Epoch 130/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 60.6920 - val_loss: 60.6159\n",
            "Epoch 131/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 60.3766 - val_loss: 60.2099\n",
            "Epoch 132/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 59.9765 - val_loss: 59.6145\n",
            "Epoch 133/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 59.6746 - val_loss: 59.1590\n",
            "Epoch 134/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 59.2187 - val_loss: 58.5618\n",
            "Epoch 135/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 58.4294 - val_loss: 58.1106\n",
            "Epoch 136/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 58.1338 - val_loss: 57.6593\n",
            "Epoch 137/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 57.4714 - val_loss: 57.1874\n",
            "Epoch 138/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 57.1217 - val_loss: 56.7041\n",
            "Epoch 139/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 56.6560 - val_loss: 56.2778\n",
            "Epoch 140/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 56.2813 - val_loss: 55.7279\n",
            "Epoch 141/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 55.6262 - val_loss: 55.2943\n",
            "Epoch 142/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 55.3661 - val_loss: 54.9101\n",
            "Epoch 143/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 54.7691 - val_loss: 54.3960\n",
            "Epoch 144/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 54.3453 - val_loss: 53.9789\n",
            "Epoch 145/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 53.9419 - val_loss: 53.4605\n",
            "Epoch 146/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 53.3586 - val_loss: 52.9862\n",
            "Epoch 147/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 53.1731 - val_loss: 52.5770\n",
            "Epoch 148/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 52.6208 - val_loss: 52.1267\n",
            "Epoch 149/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 52.1795 - val_loss: 51.6751\n",
            "Epoch 150/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 51.7258 - val_loss: 51.2258\n",
            "Epoch 151/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 51.2067 - val_loss: 50.8383\n",
            "Epoch 152/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 50.6730 - val_loss: 50.3460\n",
            "Epoch 153/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 50.3055 - val_loss: 49.8778\n",
            "Epoch 154/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 50.0301 - val_loss: 49.4212\n",
            "Epoch 155/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 49.4874 - val_loss: 49.0157\n",
            "Epoch 156/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 49.0025 - val_loss: 48.6674\n",
            "Epoch 157/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 48.8303 - val_loss: 48.2006\n",
            "Epoch 158/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 48.3329 - val_loss: 47.7784\n",
            "Epoch 159/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 47.7015 - val_loss: 47.3459\n",
            "Epoch 160/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 47.0226 - val_loss: 46.9219\n",
            "Epoch 161/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 46.8631 - val_loss: 46.4972\n",
            "Epoch 162/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 46.2081 - val_loss: 46.0837\n",
            "Epoch 163/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 46.0646 - val_loss: 45.6453\n",
            "Epoch 164/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 45.4770 - val_loss: 45.2246\n",
            "Epoch 165/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 45.0911 - val_loss: 44.8268\n",
            "Epoch 166/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 44.8286 - val_loss: 44.3649\n",
            "Epoch 167/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 44.3355 - val_loss: 44.0505\n",
            "Epoch 168/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 44.0320 - val_loss: 43.6036\n",
            "Epoch 169/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 43.5827 - val_loss: 43.1848\n",
            "Epoch 170/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 43.2831 - val_loss: 42.8003\n",
            "Epoch 171/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 42.9076 - val_loss: 42.3739\n",
            "Epoch 172/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 42.3148 - val_loss: 41.9699\n",
            "Epoch 173/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 41.9067 - val_loss: 41.6242\n",
            "Epoch 174/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 41.5668 - val_loss: 41.1927\n",
            "Epoch 175/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 41.0459 - val_loss: 40.8652\n",
            "Epoch 176/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 40.8626 - val_loss: 40.4486\n",
            "Epoch 177/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 40.3629 - val_loss: 40.0668\n",
            "Epoch 178/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 40.1515 - val_loss: 39.6637\n",
            "Epoch 179/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 39.7850 - val_loss: 39.2821\n",
            "Epoch 180/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 39.3488 - val_loss: 38.9009\n",
            "Epoch 181/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 38.7038 - val_loss: 38.5225\n",
            "Epoch 182/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 38.6297 - val_loss: 38.1007\n",
            "Epoch 183/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 37.8898 - val_loss: 37.7487\n",
            "Epoch 184/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 37.6590 - val_loss: 37.3346\n",
            "Epoch 185/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 37.4159 - val_loss: 36.9829\n",
            "Epoch 186/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 36.9356 - val_loss: 36.6659\n",
            "Epoch 187/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 36.6800 - val_loss: 36.2635\n",
            "Epoch 188/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 36.1861 - val_loss: 35.9302\n",
            "Epoch 189/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 35.8401 - val_loss: 35.5335\n",
            "Epoch 190/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 35.4120 - val_loss: 35.1476\n",
            "Epoch 191/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 35.0758 - val_loss: 34.8539\n",
            "Epoch 192/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 34.8144 - val_loss: 34.4754\n",
            "Epoch 193/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 34.4902 - val_loss: 34.1083\n",
            "Epoch 194/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 34.0013 - val_loss: 33.7563\n",
            "Epoch 195/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 33.8031 - val_loss: 33.4186\n",
            "Epoch 196/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 33.4926 - val_loss: 33.0465\n",
            "Epoch 197/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 32.9720 - val_loss: 32.7349\n",
            "Epoch 198/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 32.7942 - val_loss: 32.4195\n",
            "Epoch 199/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 32.4846 - val_loss: 32.0394\n",
            "Epoch 200/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 32.0578 - val_loss: 31.7202\n",
            "Epoch 201/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 31.4491 - val_loss: 31.3941\n",
            "Epoch 202/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 31.2711 - val_loss: 31.0859\n",
            "Epoch 203/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 31.1950 - val_loss: 30.7400\n",
            "Epoch 204/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 30.6200 - val_loss: 30.3763\n",
            "Epoch 205/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 30.2970 - val_loss: 30.0453\n",
            "Epoch 206/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 30.1398 - val_loss: 29.7095\n",
            "Epoch 207/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 29.4910 - val_loss: 29.3984\n",
            "Epoch 208/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 29.3778 - val_loss: 29.0931\n",
            "Epoch 209/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 29.1021 - val_loss: 28.7365\n",
            "Epoch 210/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 28.8550 - val_loss: 28.4112\n",
            "Epoch 211/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 28.4333 - val_loss: 28.1315\n",
            "Epoch 212/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 28.0405 - val_loss: 27.8018\n",
            "Epoch 213/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 27.8556 - val_loss: 27.4885\n",
            "Epoch 214/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 27.4901 - val_loss: 27.1635\n",
            "Epoch 215/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 27.1471 - val_loss: 26.8504\n",
            "Epoch 216/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 26.9262 - val_loss: 26.5515\n",
            "Epoch 217/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 26.6204 - val_loss: 26.2417\n",
            "Epoch 218/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 26.3068 - val_loss: 25.9601\n",
            "Epoch 219/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 25.9918 - val_loss: 25.6542\n",
            "Epoch 220/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 25.6566 - val_loss: 25.3543\n",
            "Epoch 221/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 25.4434 - val_loss: 25.0507\n",
            "Epoch 222/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 24.9482 - val_loss: 24.7571\n",
            "Epoch 223/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 24.6060 - val_loss: 24.4625\n",
            "Epoch 224/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 24.3313 - val_loss: 24.1908\n",
            "Epoch 225/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 24.2125 - val_loss: 23.8734\n",
            "Epoch 226/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 23.9685 - val_loss: 23.6193\n",
            "Epoch 227/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 23.5798 - val_loss: 23.3088\n",
            "Epoch 228/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 23.4108 - val_loss: 23.0196\n",
            "Epoch 229/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 23.1749 - val_loss: 22.7466\n",
            "Epoch 230/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 22.7361 - val_loss: 22.4601\n",
            "Epoch 231/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 22.3805 - val_loss: 22.1951\n",
            "Epoch 232/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 22.3399 - val_loss: 21.9177\n",
            "Epoch 233/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 21.9076 - val_loss: 21.6532\n",
            "Epoch 234/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 21.5417 - val_loss: 21.3847\n",
            "Epoch 235/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 21.5913 - val_loss: 21.1509\n",
            "Epoch 236/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 21.2742 - val_loss: 20.8579\n",
            "Epoch 237/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 20.8880 - val_loss: 20.6019\n",
            "Epoch 238/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 20.4488 - val_loss: 20.3266\n",
            "Epoch 239/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 20.2863 - val_loss: 20.1025\n",
            "Epoch 240/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 20.1290 - val_loss: 19.7849\n",
            "Epoch 241/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 19.8429 - val_loss: 19.5399\n",
            "Epoch 242/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 19.4743 - val_loss: 19.2848\n",
            "Epoch 243/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 19.2785 - val_loss: 19.0896\n",
            "Epoch 244/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 19.0471 - val_loss: 18.7913\n",
            "Epoch 245/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 18.7795 - val_loss: 18.5623\n",
            "Epoch 246/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 18.7414 - val_loss: 18.2935\n",
            "Epoch 247/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 18.2045 - val_loss: 18.0400\n",
            "Epoch 248/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 17.9679 - val_loss: 17.8104\n",
            "Epoch 249/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 17.9912 - val_loss: 17.5830\n",
            "Epoch 250/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 17.7669 - val_loss: 17.3531\n",
            "Epoch 251/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 17.5044 - val_loss: 17.0690\n",
            "Epoch 252/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 17.0841 - val_loss: 16.8388\n",
            "Epoch 253/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 16.9056 - val_loss: 16.6246\n",
            "Epoch 254/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 16.8597 - val_loss: 16.3996\n",
            "Epoch 255/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 16.4168 - val_loss: 16.1726\n",
            "Epoch 256/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 16.1293 - val_loss: 15.9131\n",
            "Epoch 257/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 15.9033 - val_loss: 15.7209\n",
            "Epoch 258/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 15.7512 - val_loss: 15.4699\n",
            "Epoch 259/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 15.5767 - val_loss: 15.2656\n",
            "Epoch 260/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 15.2309 - val_loss: 15.0340\n",
            "Epoch 261/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 14.8972 - val_loss: 14.8486\n",
            "Epoch 262/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 14.9862 - val_loss: 14.6062\n",
            "Epoch 263/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 14.5007 - val_loss: 14.3745\n",
            "Epoch 264/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 14.4069 - val_loss: 14.1518\n",
            "Epoch 265/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 14.2139 - val_loss: 13.9642\n",
            "Epoch 266/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 13.8957 - val_loss: 13.7643\n",
            "Epoch 267/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 13.8377 - val_loss: 13.5526\n",
            "Epoch 268/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 13.6823 - val_loss: 13.3450\n",
            "Epoch 269/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 13.2830 - val_loss: 13.1230\n",
            "Epoch 270/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 13.1468 - val_loss: 12.9367\n",
            "Epoch 271/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 12.8965 - val_loss: 12.7544\n",
            "Epoch 272/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 12.8843 - val_loss: 12.5526\n",
            "Epoch 273/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 12.5671 - val_loss: 12.3485\n",
            "Epoch 274/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 12.2620 - val_loss: 12.1428\n",
            "Epoch 275/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 12.0131 - val_loss: 11.9676\n",
            "Epoch 276/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 11.9712 - val_loss: 11.7822\n",
            "Epoch 277/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 11.9476 - val_loss: 11.5930\n",
            "Epoch 278/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 11.5906 - val_loss: 11.3988\n",
            "Epoch 279/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 11.3269 - val_loss: 11.2037\n",
            "Epoch 280/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 11.3478 - val_loss: 11.0419\n",
            "Epoch 281/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 10.9377 - val_loss: 10.8534\n",
            "Epoch 282/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 10.9212 - val_loss: 10.6758\n",
            "Epoch 283/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 10.6055 - val_loss: 10.4914\n",
            "Epoch 284/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 10.5547 - val_loss: 10.3154\n",
            "Epoch 285/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 10.3777 - val_loss: 10.1313\n",
            "Epoch 286/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 10.0928 - val_loss: 9.9708\n",
            "Epoch 287/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 10.0448 - val_loss: 9.8030\n",
            "Epoch 288/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 9.8458 - val_loss: 9.6295\n",
            "Epoch 289/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 9.7308 - val_loss: 9.4853\n",
            "Epoch 290/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 9.4803 - val_loss: 9.2995\n",
            "Epoch 291/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 9.2407 - val_loss: 9.1394\n",
            "Epoch 292/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 9.1749 - val_loss: 8.9841\n",
            "Epoch 293/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 9.0624 - val_loss: 8.8220\n",
            "Epoch 294/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 8.9744 - val_loss: 8.6533\n",
            "Epoch 295/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 8.6330 - val_loss: 8.5028\n",
            "Epoch 296/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 8.4214 - val_loss: 8.3520\n",
            "Epoch 297/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 8.3728 - val_loss: 8.2175\n",
            "Epoch 298/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 8.2323 - val_loss: 8.0696\n",
            "Epoch 299/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 8.1013 - val_loss: 7.8976\n",
            "Epoch 300/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 7.7899 - val_loss: 7.7798\n",
            "Epoch 301/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 7.8040 - val_loss: 7.5946\n",
            "Epoch 302/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 7.6655 - val_loss: 7.4564\n",
            "Epoch 303/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 7.4212 - val_loss: 7.3092\n",
            "Epoch 304/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 7.3030 - val_loss: 7.1759\n",
            "Epoch 305/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 7.2309 - val_loss: 7.0391\n",
            "Epoch 306/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 7.1086 - val_loss: 6.8957\n",
            "Epoch 307/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 6.9383 - val_loss: 6.7746\n",
            "Epoch 308/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 6.8228 - val_loss: 6.6359\n",
            "Epoch 309/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 6.6053 - val_loss: 6.5056\n",
            "Epoch 310/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 6.4914 - val_loss: 6.3845\n",
            "Epoch 311/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 6.3891 - val_loss: 6.2543\n",
            "Epoch 312/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 6.3624 - val_loss: 6.1221\n",
            "Epoch 313/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 6.1511 - val_loss: 5.9930\n",
            "Epoch 314/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 5.9392 - val_loss: 5.8761\n",
            "Epoch 315/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 5.8442 - val_loss: 5.7506\n",
            "Epoch 316/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 5.7148 - val_loss: 5.6310\n",
            "Epoch 317/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 5.5914 - val_loss: 5.5164\n",
            "Epoch 318/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 5.5684 - val_loss: 5.3783\n",
            "Epoch 319/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 5.3615 - val_loss: 5.2815\n",
            "Epoch 320/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 5.3577 - val_loss: 5.1607\n",
            "Epoch 321/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 5.1263 - val_loss: 5.0533\n",
            "Epoch 322/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 5.0496 - val_loss: 4.9382\n",
            "Epoch 323/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 4.9515 - val_loss: 4.8254\n",
            "Epoch 324/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 4.8934 - val_loss: 4.7226\n",
            "Epoch 325/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 4.8329 - val_loss: 4.6146\n",
            "Epoch 326/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 4.7134 - val_loss: 4.5131\n",
            "Epoch 327/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 4.4708 - val_loss: 4.4157\n",
            "Epoch 328/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 4.4237 - val_loss: 4.3156\n",
            "Epoch 329/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 4.3043 - val_loss: 4.2168\n",
            "Epoch 330/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 4.2244 - val_loss: 4.1124\n",
            "Epoch 331/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 4.1454 - val_loss: 4.0218\n",
            "Epoch 332/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 4.0677 - val_loss: 3.9277\n",
            "Epoch 333/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 4.0298 - val_loss: 3.8356\n",
            "Epoch 334/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 3.8100 - val_loss: 3.7484\n",
            "Epoch 335/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 3.7125 - val_loss: 3.6487\n",
            "Epoch 336/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 3.6683 - val_loss: 3.5698\n",
            "Epoch 337/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 3.6876 - val_loss: 3.4681\n",
            "Epoch 338/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 3.4407 - val_loss: 3.3821\n",
            "Epoch 339/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 3.3890 - val_loss: 3.2964\n",
            "Epoch 340/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 3.3220 - val_loss: 3.2199\n",
            "Epoch 341/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 3.2014 - val_loss: 3.1245\n",
            "Epoch 342/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 3.1748 - val_loss: 3.0596\n",
            "Epoch 343/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 3.0753 - val_loss: 2.9694\n",
            "Epoch 344/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 2.9806 - val_loss: 2.8981\n",
            "Epoch 345/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 2.9429 - val_loss: 2.8162\n",
            "Epoch 346/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 2.8195 - val_loss: 2.7466\n",
            "Epoch 347/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 2.8082 - val_loss: 2.6765\n",
            "Epoch 348/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 2.6583 - val_loss: 2.6077\n",
            "Epoch 349/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 2.6298 - val_loss: 2.5345\n",
            "Epoch 350/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 2.5377 - val_loss: 2.4559\n",
            "Epoch 351/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 2.4156 - val_loss: 2.3942\n",
            "Epoch 352/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 2.5118 - val_loss: 2.3236\n",
            "Epoch 353/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 2.2676 - val_loss: 2.2660\n",
            "Epoch 354/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 2.2741 - val_loss: 2.1952\n",
            "Epoch 355/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 2.1793 - val_loss: 2.1347\n",
            "Epoch 356/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 2.1688 - val_loss: 2.0732\n",
            "Epoch 357/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 2.0696 - val_loss: 2.0172\n",
            "Epoch 358/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 2.0601 - val_loss: 1.9550\n",
            "Epoch 359/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 1.9758 - val_loss: 1.8958\n",
            "Epoch 360/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 1.9264 - val_loss: 1.8401\n",
            "Epoch 361/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 1.8453 - val_loss: 1.7881\n",
            "Epoch 362/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 1.8121 - val_loss: 1.7377\n",
            "Epoch 363/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 1.7485 - val_loss: 1.6816\n",
            "Epoch 364/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 1.6739 - val_loss: 1.6296\n",
            "Epoch 365/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 1.6377 - val_loss: 1.5806\n",
            "Epoch 366/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 1.6201 - val_loss: 1.5341\n",
            "Epoch 367/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 1.5296 - val_loss: 1.4824\n",
            "Epoch 368/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 1.5245 - val_loss: 1.4373\n",
            "Epoch 369/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 1.4333 - val_loss: 1.3885\n",
            "Epoch 370/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 1.4662 - val_loss: 1.3457\n",
            "Epoch 371/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 1.3888 - val_loss: 1.3022\n",
            "Epoch 372/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 1.2947 - val_loss: 1.2691\n",
            "Epoch 373/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 1.2538 - val_loss: 1.2228\n",
            "Epoch 374/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 1.2691 - val_loss: 1.1800\n",
            "Epoch 375/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 1.2345 - val_loss: 1.1407\n",
            "Epoch 376/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 1.1631 - val_loss: 1.1081\n",
            "Epoch 377/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 1.1229 - val_loss: 1.0701\n",
            "Epoch 378/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 1.0845 - val_loss: 1.0327\n",
            "Epoch 379/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 1.0310 - val_loss: 0.9989\n",
            "Epoch 380/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 1.0517 - val_loss: 0.9649\n",
            "Epoch 381/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.9568 - val_loss: 0.9372\n",
            "Epoch 382/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.9619 - val_loss: 0.9002\n",
            "Epoch 383/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.9624 - val_loss: 0.8679\n",
            "Epoch 384/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.9108 - val_loss: 0.8405\n",
            "Epoch 385/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.8483 - val_loss: 0.8173\n",
            "Epoch 386/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.8568 - val_loss: 0.7848\n",
            "Epoch 387/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.8274 - val_loss: 0.7606\n",
            "Epoch 388/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.8123 - val_loss: 0.7301\n",
            "Epoch 389/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.7572 - val_loss: 0.7062\n",
            "Epoch 390/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.7143 - val_loss: 0.6820\n",
            "Epoch 391/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.7051 - val_loss: 0.6577\n",
            "Epoch 392/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.7129 - val_loss: 0.6348\n",
            "Epoch 393/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.7029 - val_loss: 0.6100\n",
            "Epoch 394/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.6573 - val_loss: 0.5862\n",
            "Epoch 395/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.6053 - val_loss: 0.5756\n",
            "Epoch 396/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.5858 - val_loss: 0.5555\n",
            "Epoch 397/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.5549 - val_loss: 0.5359\n",
            "Epoch 398/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.5349 - val_loss: 0.5162\n",
            "Epoch 399/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.5255 - val_loss: 0.5018\n",
            "Epoch 400/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.5374 - val_loss: 0.4798\n",
            "Epoch 401/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.5357 - val_loss: 0.4660\n",
            "Epoch 402/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.5060 - val_loss: 0.4502\n",
            "Epoch 403/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.4738 - val_loss: 0.4329\n",
            "Epoch 404/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.4474 - val_loss: 0.4220\n",
            "Epoch 405/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.4488 - val_loss: 0.4086\n",
            "Epoch 406/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.4323 - val_loss: 0.3952\n",
            "Epoch 407/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.4346 - val_loss: 0.3799\n",
            "Epoch 408/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.4190 - val_loss: 0.3739\n",
            "Epoch 409/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.3992 - val_loss: 0.3589\n",
            "Epoch 410/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.3945 - val_loss: 0.3460\n",
            "Epoch 411/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.3703 - val_loss: 0.3401\n",
            "Epoch 412/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.3613 - val_loss: 0.3320\n",
            "Epoch 413/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.3495 - val_loss: 0.3147\n",
            "Epoch 414/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.3562 - val_loss: 0.3119\n",
            "Epoch 415/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.3475 - val_loss: 0.3001\n",
            "Epoch 416/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.3259 - val_loss: 0.2972\n",
            "Epoch 417/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.3182 - val_loss: 0.2901\n",
            "Epoch 418/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.3185 - val_loss: 0.2768\n",
            "Epoch 419/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.2969 - val_loss: 0.2717\n",
            "Epoch 420/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.2934 - val_loss: 0.2696\n",
            "Epoch 421/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.2933 - val_loss: 0.2587\n",
            "Epoch 422/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.2840 - val_loss: 0.2493\n",
            "Epoch 423/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.2734 - val_loss: 0.2491\n",
            "Epoch 424/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.2805 - val_loss: 0.2481\n",
            "Epoch 425/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.2779 - val_loss: 0.2394\n",
            "Epoch 426/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.2610 - val_loss: 0.2424\n",
            "Epoch 427/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.2634 - val_loss: 0.2366\n",
            "Epoch 428/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.2747 - val_loss: 0.2292\n",
            "Epoch 429/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.2582 - val_loss: 0.2215\n",
            "Epoch 430/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.2641 - val_loss: 0.2179\n",
            "Epoch 431/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.2536 - val_loss: 0.2212\n",
            "Epoch 432/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.2426 - val_loss: 0.2137\n",
            "Epoch 433/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.2449 - val_loss: 0.2153\n",
            "Epoch 434/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.2539 - val_loss: 0.2104\n",
            "Epoch 435/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.2526 - val_loss: 0.2089\n",
            "Epoch 436/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.2372 - val_loss: 0.2084\n",
            "Epoch 437/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.2305 - val_loss: 0.2051\n",
            "Epoch 438/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.2171 - val_loss: 0.1982\n",
            "Epoch 439/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.2353 - val_loss: 0.2001\n",
            "Epoch 440/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.2335 - val_loss: 0.1978\n",
            "Epoch 441/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.2263 - val_loss: 0.2002\n",
            "Epoch 442/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.2280 - val_loss: 0.1979\n",
            "Epoch 443/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.2167 - val_loss: 0.1975\n",
            "Epoch 444/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.2176 - val_loss: 0.1973\n",
            "Epoch 445/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.2285 - val_loss: 0.1976\n",
            "Epoch 446/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.2139 - val_loss: 0.1962\n",
            "Epoch 447/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.2229 - val_loss: 0.1908\n",
            "Epoch 448/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.2054 - val_loss: 0.1943\n",
            "Epoch 449/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.2250 - val_loss: 0.1947\n",
            "Epoch 450/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.2120 - val_loss: 0.1951\n",
            "Epoch 451/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.2260 - val_loss: 0.1877\n",
            "Epoch 452/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.2332 - val_loss: 0.1913\n",
            "Epoch 453/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.2195 - val_loss: 0.1889\n",
            "Epoch 454/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.2235 - val_loss: 0.1873\n",
            "Epoch 455/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.2058 - val_loss: 0.1875\n",
            "Epoch 456/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.2135 - val_loss: 0.1889\n",
            "Epoch 457/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.2150 - val_loss: 0.1903\n",
            "Epoch 458/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.2288 - val_loss: 0.1881\n",
            "Epoch 459/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.2163 - val_loss: 0.1872\n",
            "Epoch 460/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.2174 - val_loss: 0.1850\n",
            "Epoch 461/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.2248 - val_loss: 0.1882\n",
            "Epoch 462/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.2137 - val_loss: 0.1854\n",
            "Epoch 463/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.2129 - val_loss: 0.1877\n",
            "Epoch 464/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.2065 - val_loss: 0.1921\n",
            "Epoch 465/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.2144 - val_loss: 0.1915\n",
            "Epoch 466/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.2344 - val_loss: 0.1858\n",
            "Epoch 467/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.2095 - val_loss: 0.1896\n",
            "Epoch 468/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.2268 - val_loss: 0.1880\n",
            "Epoch 469/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.2139 - val_loss: 0.1871\n",
            "Epoch 470/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.2176 - val_loss: 0.1876\n",
            "Epoch 471/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.2288 - val_loss: 0.1898\n",
            "Epoch 472/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.2124 - val_loss: 0.1872\n",
            "Epoch 473/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.2130 - val_loss: 0.1924\n",
            "Epoch 474/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.2278 - val_loss: 0.1893\n",
            "Epoch 475/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.2135 - val_loss: 0.1861\n",
            "Epoch 476/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.2136 - val_loss: 0.1876\n",
            "Epoch 477/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.2287 - val_loss: 0.1799\n",
            "Epoch 478/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.2035 - val_loss: 0.1881\n",
            "Epoch 479/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.2139 - val_loss: 0.1860\n",
            "Epoch 480/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.2011 - val_loss: 0.1884\n",
            "Epoch 481/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.2157 - val_loss: 0.1820\n",
            "Epoch 482/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.2053 - val_loss: 0.1862\n",
            "Epoch 483/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.2294 - val_loss: 0.1867\n",
            "Epoch 484/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.2056 - val_loss: 0.1855\n",
            "Epoch 485/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.2128 - val_loss: 0.1922\n",
            "Epoch 486/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.2076 - val_loss: 0.1872\n",
            "Epoch 487/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.2164 - val_loss: 0.1855\n",
            "Epoch 488/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.2053 - val_loss: 0.1871\n",
            "Epoch 489/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.2216 - val_loss: 0.1844\n",
            "Epoch 490/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.2167 - val_loss: 0.1866\n",
            "Epoch 491/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.2205 - val_loss: 0.1883\n",
            "Epoch 492/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.2181 - val_loss: 0.1871\n",
            "Epoch 493/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.2122 - val_loss: 0.1896\n",
            "Epoch 494/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.2194 - val_loss: 0.1856\n",
            "Epoch 495/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.2159 - val_loss: 0.1877\n",
            "Epoch 496/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.2214 - val_loss: 0.1890\n",
            "Epoch 497/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.2240 - val_loss: 0.1885\n",
            "Epoch 498/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.2049 - val_loss: 0.1854\n",
            "Epoch 499/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.2146 - val_loss: 0.1906\n",
            "Epoch 500/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.1967 - val_loss: 0.1932\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AtdUbgUd4Ifw"
      },
      "source": [
        "And this is how you can predict an output for any number of inputs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KaO-wCLl3Wd_",
        "outputId": "f4c2a210-9648-4fbd-a7de-746bf9cb8acb"
      },
      "source": [
        "print(model.predict(X_test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[12.66367 ]\n",
            " [12.601315]\n",
            " [12.644816]\n",
            " ...\n",
            " [12.637957]\n",
            " [12.566677]\n",
            " [12.670867]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y6VhmPDN4Sva"
      },
      "source": [
        "### 1.1.2.7 Visualize Results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7bDCQffM4Zh2"
      },
      "source": [
        "It's time to see how your model's progress during the training, If all is good, you will find the validation loss decreasing without neither overfitting nor underfitting."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CEZ7SdwI2e_D",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 278
        },
        "outputId": "835d66c5-3746-4c4c-8aab-e3b947eab1f1"
      },
      "source": [
        "# Get training and test loss histories\n",
        "training_loss = hist.history['loss']\n",
        "val_loss = hist.history['val_loss']\n",
        "\n",
        "# Create count of the number of epochs\n",
        "epoch_count = range(1, len(training_loss) + 1)\n",
        "\n",
        "# Visualize loss history\n",
        "plt.figure()\n",
        "plt.plot(epoch_count, training_loss, 'r--')\n",
        "plt.plot(epoch_count, val_loss, 'b-')\n",
        "plt.legend(['Training Loss', 'Val Loss'])\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "image/png": {
              "width": 388,
              "height": 261
            },
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6gni9AWDueGU"
      },
      "source": [
        "## 1.1.3 Conclusion:\n",
        "\n",
        "That's it! Congratulations on training a linear regression model. \n",
        "\n",
        "Make sure you finish the second part of the assignment and deliver all the requirements for the submission.\n",
        "\n"
      ]
    }
  ]
}
