{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of lab1_part2.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dareenhussein/PMDL/blob/main/Copy_of_lab1_part2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0bxGkToGpFrJ"
      },
      "source": [
        "<table align=\"center\">\n",
        "  <td align=\"center\"><a target=\"_blank\" href=\"https://colab.research.google.com/github/KhaledElTahan/DeepLearning/blob/master/Labs/lab1/lab1_part2.ipynb\">\n",
        "        <img src=\"http://introtodeeplearning.com/images/colab/colab.png?v2.0\"  style=\"padding-bottom:5px;\" />Run in Google Colab</a></td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C6OFYzfbpHX6"
      },
      "source": [
        "# Copyright Information\n",
        "\n",
        "**Parts of this lab are based on Kaggle kernels.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hA5NHvtepVUX"
      },
      "source": [
        "# Lab 1 - Part2: Logistic Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-JgGI_Q82Dor"
      },
      "source": [
        "![Logistic Regression](https://raw.githubusercontent.com/KhaledElTahan/AUC-DeepLearning/master/Labs/lab1/logistic_regression.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LbWcnTT02NPV"
      },
      "source": [
        "## 1.2.1 Problem Statement"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lhU6-7Mx2TVB"
      },
      "source": [
        "Here, we are trying to increase the peoplesâ€™ attention regarding the heart diseases. Like any disease, it is always better to know if you are sick early so you can get the treatment you need before it is too late. Therefore, we use a dataset that gathered some information about two groups: a group with a heart disease and the other group has no disease.\n",
        "The gathered information includes age, chest pain type, fasting blood sugar, etc. \n",
        "\n",
        "Your goal is to train a logistic regression model to predict if a person has a heart disease or not depending on the given information."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2PTcUN4K24pK"
      },
      "source": [
        "## 1.2.2 Problem Details"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ofHL4e073Ca0"
      },
      "source": [
        "Let's dive into the code, explain it and show you the parts you need to fill!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sjQNwFqC4NEM"
      },
      "source": [
        "### 1.2.2.1 Import Needed packages"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I0AnpII84Pdw"
      },
      "source": [
        "Pay close attention to the packages I imported for you."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L0_d4RG4zcC_"
      },
      "source": [
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Activation\n",
        "from tensorflow.keras.optimizers import Adam, SGD, Adagrad, RMSprop\n",
        "from tensorflow.keras.initializers import RandomNormal, RandomUniform\n",
        "from tensorflow.keras.losses import BinaryCrossentropy, CategoricalHinge\n",
        "from tensorflow.keras import regularizers\n",
        "from sklearn import preprocessing"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ePGBlsWk4ner"
      },
      "source": [
        "### 1.2.2.2 Work on the dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fR6WYKSo5tRH"
      },
      "source": [
        "This dataset contains 13 features that demonstrate the health state of a person and our target (0 if this person does not have a heart disease and 1 if he has a heart disease.)\n",
        "\n",
        "We first load the dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PRxqN_IPo6Fg"
      },
      "source": [
        "dataset = pd.read_csv(\"https://raw.githubusercontent.com/KhaledElTahan/DeepLearning/master/Labs/lab1/lab1_heart.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yGsChzk_55nr"
      },
      "source": [
        "A sneak peak on the dataset and how it looks like."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jRQkXsiHrTBB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "1c624a58-ce33-4b9e-e7cb-74bc1a0764de"
      },
      "source": [
        "dataset.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>age</th>\n",
              "      <th>sex</th>\n",
              "      <th>cp</th>\n",
              "      <th>trestbps</th>\n",
              "      <th>chol</th>\n",
              "      <th>fbs</th>\n",
              "      <th>restecg</th>\n",
              "      <th>thalach</th>\n",
              "      <th>exang</th>\n",
              "      <th>oldpeak</th>\n",
              "      <th>slope</th>\n",
              "      <th>ca</th>\n",
              "      <th>thal</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>63</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>145</td>\n",
              "      <td>233</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>150</td>\n",
              "      <td>0</td>\n",
              "      <td>2.3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>37</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>130</td>\n",
              "      <td>250</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>187</td>\n",
              "      <td>0</td>\n",
              "      <td>3.5</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>41</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>130</td>\n",
              "      <td>204</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>172</td>\n",
              "      <td>0</td>\n",
              "      <td>1.4</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>56</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>120</td>\n",
              "      <td>236</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>178</td>\n",
              "      <td>0</td>\n",
              "      <td>0.8</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>57</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>120</td>\n",
              "      <td>354</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>163</td>\n",
              "      <td>1</td>\n",
              "      <td>0.6</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   age  sex  cp  trestbps  chol  fbs  ...  exang  oldpeak  slope  ca  thal  target\n",
              "0   63    1   3       145   233    1  ...      0      2.3      0   0     1       1\n",
              "1   37    1   2       130   250    0  ...      0      3.5      0   0     2       1\n",
              "2   41    0   1       130   204    0  ...      0      1.4      2   0     2       1\n",
              "3   56    1   1       120   236    0  ...      0      0.8      2   0     2       1\n",
              "4   57    0   0       120   354    0  ...      1      0.6      2   0     2       1\n",
              "\n",
              "[5 rows x 14 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yJa4KJEV6AFm"
      },
      "source": [
        "Define input and output."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I1UDjkHZrbey"
      },
      "source": [
        "X = dataset.iloc[:, 0:13].values\n",
        "y = dataset.iloc[:, 13].values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uTd_AkXK6Y1G"
      },
      "source": [
        "**TODO: Preprocess your data**\n",
        "\n",
        "1.   Do you need to scale the data? Which type of scaling is better? \n",
        "2.   Perhaps you might want to add non-linearity by adding artificial features.\n",
        "\n",
        "![The effect of the boundary with artificial features](https://raw.githubusercontent.com/KhaledElTahan/AUC-DeepLearning/master/Labs/lab1/artificial_features_boundaries.png)\n",
        "\n",
        "You might have a look on part1 preprocessing and take hints from there. \n",
        "\n",
        "**Try different types of data preprocessing and include their effect on the accuracy in your report.**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RyDjY4ECpenA"
      },
      "source": [
        "# TODO: Data Pre-Processing\n",
        "#Artificial feature\n",
        "cubic_X = X ** 3\n",
        "X = np.concatenate((X, cubic_X), axis=1)\n",
        "\n",
        "# Standardize features by removing the mean and scaling to unit variance\n",
        "# z = (x - u) / s\n",
        "#X = StandardScaler().fit_transform(X)\n",
        "min_max_scaler = preprocessing.MinMaxScaler()\n",
        "X = min_max_scaler.fit_transform(X)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-r81XUakEtRi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c2bce3b9-cd45-45b8-9830-aa3dc27764c4"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pv6Cy9ar6ewP"
      },
      "source": [
        "Split dataset into, training, validation and testing splits."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NCBELT8wpsrq"
      },
      "source": [
        "# Get Training Data\n",
        "train_X, temporary_X, train_y, temporary_y = train_test_split(X, y, train_size=0.75, random_state=0)\n",
        "\n",
        "# Get Validation & Testing Data\n",
        "val_X, test_X, val_y, test_y = train_test_split(temporary_X, temporary_y, train_size=0.5, random_state=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tk5uv_H26p5M"
      },
      "source": [
        "### 1.2.2.3 Define your model here (TODO)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3eWXVX966kiM"
      },
      "source": [
        "Logistic Regression as a model is exactly like the Linear Regression except for the activation function. \n",
        "\n",
        "Use this fact to define your model similar to part1 except for the actication function.\n",
        "\n",
        "![Logistic Regression using Simple Perceptron](https://raw.githubusercontent.com/KhaledElTahan/AUC-DeepLearning/master/Labs/lab1/perceptron_activation.png)\n",
        "\n",
        "**TODO**: \n",
        "1. Try different activation functions and include in the report their effect on the accuracy.\n",
        "2. Try different regularizers and include in the report their effect on the accuracy.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tqJwVMxdptTz"
      },
      "source": [
        "#activation = 'relu'\n",
        "#regularizer = regularizers.l2(0.01)\n",
        "# TODO: Define the Model using Tensorflow.Keras\n",
        "model = Sequential([Dense(1,input_shape=(26,), activation = 'sigmoid', \n",
        "                                          kernel_regularizer = regularizers.l2(0.01))])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jFHo5TzW_yCw"
      },
      "source": [
        "### 1.2.2.4 Compile your model and print a summary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xoaNNu-z6m2j"
      },
      "source": [
        "**TODO**\n",
        "1. Try different losses functions and include in the report their effect on the accuracy. Make sure that those losses functions are meant only for classification! Don't use losses functions that are meant for prediction!\n",
        "2. Try different optimizers and include in the report their effect on the accuracy and the training plot."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i5WXiT2kpv66",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a4d2542f-bb47-4c3c-a0ba-a02910016039"
      },
      "source": [
        "## TODO Try Different losses & optimizers here\n",
        "model.compile(loss = BinaryCrossentropy(), metrics=['accuracy'], optimizer = Adam())\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense (Dense)                (None, 1)                 27        \n",
            "=================================================================\n",
            "Total params: 27\n",
            "Trainable params: 27\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tcTmEu8AAA4W"
      },
      "source": [
        "### 1.2.2.5 Train your model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wBhupRN6_39e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "083f00c7-d6d7-42e4-e169-67253f1b8f8b"
      },
      "source": [
        "hist = model.fit(train_X, train_y, verbose=1, validation_data=(val_X, val_y), batch_size=16, epochs=500)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/500\n",
            "15/15 [==============================] - 4s 25ms/step - loss: 0.8517 - accuracy: 0.4288 - val_loss: 0.7948 - val_accuracy: 0.4737\n",
            "Epoch 2/500\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.7855 - accuracy: 0.4806 - val_loss: 0.7639 - val_accuracy: 0.4737\n",
            "Epoch 3/500\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.7639 - accuracy: 0.4532 - val_loss: 0.7380 - val_accuracy: 0.4737\n",
            "Epoch 4/500\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.7309 - accuracy: 0.4606 - val_loss: 0.7150 - val_accuracy: 0.4737\n",
            "Epoch 5/500\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.7196 - accuracy: 0.4642 - val_loss: 0.6956 - val_accuracy: 0.4737\n",
            "Epoch 6/500\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.6967 - accuracy: 0.4562 - val_loss: 0.6809 - val_accuracy: 0.5526\n",
            "Epoch 7/500\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.7056 - accuracy: 0.4416 - val_loss: 0.6676 - val_accuracy: 0.6316\n",
            "Epoch 8/500\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.6384 - accuracy: 0.5469 - val_loss: 0.6563 - val_accuracy: 0.6842\n",
            "Epoch 9/500\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.6323 - accuracy: 0.6280 - val_loss: 0.6462 - val_accuracy: 0.6842\n",
            "Epoch 10/500\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.6557 - accuracy: 0.6167 - val_loss: 0.6368 - val_accuracy: 0.6842\n",
            "Epoch 11/500\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.6477 - accuracy: 0.6459 - val_loss: 0.6286 - val_accuracy: 0.6842\n",
            "Epoch 12/500\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.6426 - accuracy: 0.6826 - val_loss: 0.6218 - val_accuracy: 0.7105\n",
            "Epoch 13/500\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.6240 - accuracy: 0.7189 - val_loss: 0.6153 - val_accuracy: 0.7368\n",
            "Epoch 14/500\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.6080 - accuracy: 0.7327 - val_loss: 0.6107 - val_accuracy: 0.7368\n",
            "Epoch 15/500\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.6057 - accuracy: 0.7673 - val_loss: 0.6060 - val_accuracy: 0.7632\n",
            "Epoch 16/500\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.6199 - accuracy: 0.7601 - val_loss: 0.6017 - val_accuracy: 0.7895\n",
            "Epoch 17/500\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.5843 - accuracy: 0.8139 - val_loss: 0.5977 - val_accuracy: 0.7895\n",
            "Epoch 18/500\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.5927 - accuracy: 0.8163 - val_loss: 0.5937 - val_accuracy: 0.7895\n",
            "Epoch 19/500\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.5892 - accuracy: 0.8061 - val_loss: 0.5899 - val_accuracy: 0.7895\n",
            "Epoch 20/500\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.5900 - accuracy: 0.7959 - val_loss: 0.5860 - val_accuracy: 0.8158\n",
            "Epoch 21/500\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.5924 - accuracy: 0.7551 - val_loss: 0.5829 - val_accuracy: 0.8158\n",
            "Epoch 22/500\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.5945 - accuracy: 0.7678 - val_loss: 0.5803 - val_accuracy: 0.8158\n",
            "Epoch 23/500\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.5642 - accuracy: 0.8144 - val_loss: 0.5774 - val_accuracy: 0.8158\n",
            "Epoch 24/500\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.5666 - accuracy: 0.8230 - val_loss: 0.5751 - val_accuracy: 0.8158\n",
            "Epoch 25/500\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.5672 - accuracy: 0.7939 - val_loss: 0.5728 - val_accuracy: 0.7895\n",
            "Epoch 26/500\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.5519 - accuracy: 0.8230 - val_loss: 0.5703 - val_accuracy: 0.7632\n",
            "Epoch 27/500\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.5647 - accuracy: 0.7801 - val_loss: 0.5682 - val_accuracy: 0.7632\n",
            "Epoch 28/500\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.5583 - accuracy: 0.7907 - val_loss: 0.5655 - val_accuracy: 0.7632\n",
            "Epoch 29/500\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.5461 - accuracy: 0.8091 - val_loss: 0.5631 - val_accuracy: 0.7632\n",
            "Epoch 30/500\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.5286 - accuracy: 0.8115 - val_loss: 0.5614 - val_accuracy: 0.7632\n",
            "Epoch 31/500\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.5599 - accuracy: 0.7927 - val_loss: 0.5598 - val_accuracy: 0.7632\n",
            "Epoch 32/500\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.5600 - accuracy: 0.7711 - val_loss: 0.5575 - val_accuracy: 0.7632\n",
            "Epoch 33/500\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.5502 - accuracy: 0.8022 - val_loss: 0.5552 - val_accuracy: 0.7632\n",
            "Epoch 34/500\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.5371 - accuracy: 0.7994 - val_loss: 0.5535 - val_accuracy: 0.7632\n",
            "Epoch 35/500\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.5511 - accuracy: 0.7809 - val_loss: 0.5518 - val_accuracy: 0.7632\n",
            "Epoch 36/500\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.5373 - accuracy: 0.8010 - val_loss: 0.5496 - val_accuracy: 0.7632\n",
            "Epoch 37/500\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.5343 - accuracy: 0.7833 - val_loss: 0.5480 - val_accuracy: 0.7632\n",
            "Epoch 38/500\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.5264 - accuracy: 0.8176 - val_loss: 0.5463 - val_accuracy: 0.7632\n",
            "Epoch 39/500\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.5323 - accuracy: 0.7926 - val_loss: 0.5448 - val_accuracy: 0.7632\n",
            "Epoch 40/500\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.5522 - accuracy: 0.7854 - val_loss: 0.5433 - val_accuracy: 0.7632\n",
            "Epoch 41/500\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.5273 - accuracy: 0.8085 - val_loss: 0.5423 - val_accuracy: 0.7632\n",
            "Epoch 42/500\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.5436 - accuracy: 0.7908 - val_loss: 0.5412 - val_accuracy: 0.7632\n",
            "Epoch 43/500\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.5178 - accuracy: 0.8349 - val_loss: 0.5400 - val_accuracy: 0.7632\n",
            "Epoch 44/500\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.5233 - accuracy: 0.8113 - val_loss: 0.5391 - val_accuracy: 0.7632\n",
            "Epoch 45/500\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.5324 - accuracy: 0.8011 - val_loss: 0.5381 - val_accuracy: 0.7632\n",
            "Epoch 46/500\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.4969 - accuracy: 0.8511 - val_loss: 0.5364 - val_accuracy: 0.7632\n",
            "Epoch 47/500\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.5253 - accuracy: 0.7812 - val_loss: 0.5351 - val_accuracy: 0.7632\n",
            "Epoch 48/500\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.5156 - accuracy: 0.7974 - val_loss: 0.5335 - val_accuracy: 0.7895\n",
            "Epoch 49/500\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.4947 - accuracy: 0.8263 - val_loss: 0.5319 - val_accuracy: 0.8158\n",
            "Epoch 50/500\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.4907 - accuracy: 0.8338 - val_loss: 0.5304 - val_accuracy: 0.8158\n",
            "Epoch 51/500\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.5073 - accuracy: 0.8039 - val_loss: 0.5294 - val_accuracy: 0.8158\n",
            "Epoch 52/500\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.5025 - accuracy: 0.8140 - val_loss: 0.5283 - val_accuracy: 0.8158\n",
            "Epoch 53/500\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.5051 - accuracy: 0.8138 - val_loss: 0.5272 - val_accuracy: 0.8158\n",
            "Epoch 54/500\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.4949 - accuracy: 0.8376 - val_loss: 0.5262 - val_accuracy: 0.8158\n",
            "Epoch 55/500\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.5186 - accuracy: 0.7956 - val_loss: 0.5247 - val_accuracy: 0.8158\n",
            "Epoch 56/500\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.4894 - accuracy: 0.8258 - val_loss: 0.5238 - val_accuracy: 0.8158\n",
            "Epoch 57/500\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.5083 - accuracy: 0.8152 - val_loss: 0.5234 - val_accuracy: 0.8158\n",
            "Epoch 58/500\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.5016 - accuracy: 0.8061 - val_loss: 0.5226 - val_accuracy: 0.8158\n",
            "Epoch 59/500\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.4896 - accuracy: 0.8445 - val_loss: 0.5220 - val_accuracy: 0.8158\n",
            "Epoch 60/500\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.5185 - accuracy: 0.7755 - val_loss: 0.5212 - val_accuracy: 0.8158\n",
            "Epoch 61/500\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.5039 - accuracy: 0.7972 - val_loss: 0.5204 - val_accuracy: 0.8158\n",
            "Epoch 62/500\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.5056 - accuracy: 0.8184 - val_loss: 0.5194 - val_accuracy: 0.8158\n",
            "Epoch 63/500\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.4868 - accuracy: 0.8453 - val_loss: 0.5184 - val_accuracy: 0.8158\n",
            "Epoch 64/500\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.5089 - accuracy: 0.8120 - val_loss: 0.5180 - val_accuracy: 0.7895\n",
            "Epoch 65/500\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.4800 - accuracy: 0.8285 - val_loss: 0.5170 - val_accuracy: 0.7895\n",
            "Epoch 66/500\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.4808 - accuracy: 0.8221 - val_loss: 0.5165 - val_accuracy: 0.7895\n",
            "Epoch 67/500\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.5049 - accuracy: 0.8007 - val_loss: 0.5150 - val_accuracy: 0.7895\n",
            "Epoch 68/500\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.4646 - accuracy: 0.8512 - val_loss: 0.5143 - val_accuracy: 0.7895\n",
            "Epoch 69/500\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.4943 - accuracy: 0.8321 - val_loss: 0.5133 - val_accuracy: 0.7895\n",
            "Epoch 70/500\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.4996 - accuracy: 0.7912 - val_loss: 0.5134 - val_accuracy: 0.7895\n",
            "Epoch 71/500\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.4891 - accuracy: 0.8167 - val_loss: 0.5121 - val_accuracy: 0.7895\n",
            "Epoch 72/500\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.4920 - accuracy: 0.7949 - val_loss: 0.5112 - val_accuracy: 0.7895\n",
            "Epoch 73/500\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.5082 - accuracy: 0.8068 - val_loss: 0.5099 - val_accuracy: 0.7895\n",
            "Epoch 74/500\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.5039 - accuracy: 0.8180 - val_loss: 0.5087 - val_accuracy: 0.8158\n",
            "Epoch 75/500\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.4860 - accuracy: 0.8248 - val_loss: 0.5086 - val_accuracy: 0.7895\n",
            "Epoch 76/500\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.4701 - accuracy: 0.8404 - val_loss: 0.5076 - val_accuracy: 0.7895\n",
            "Epoch 77/500\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.4727 - accuracy: 0.8405 - val_loss: 0.5064 - val_accuracy: 0.8158\n",
            "Epoch 78/500\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.5044 - accuracy: 0.7915 - val_loss: 0.5064 - val_accuracy: 0.7895\n",
            "Epoch 79/500\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.4915 - accuracy: 0.7919 - val_loss: 0.5062 - val_accuracy: 0.7895\n",
            "Epoch 80/500\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.4709 - accuracy: 0.8432 - val_loss: 0.5056 - val_accuracy: 0.7895\n",
            "Epoch 81/500\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.4846 - accuracy: 0.8201 - val_loss: 0.5051 - val_accuracy: 0.7895\n",
            "Epoch 82/500\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.4675 - accuracy: 0.8392 - val_loss: 0.5051 - val_accuracy: 0.7895\n",
            "Epoch 83/500\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.4822 - accuracy: 0.8318 - val_loss: 0.5047 - val_accuracy: 0.7895\n",
            "Epoch 84/500\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.5022 - accuracy: 0.8092 - val_loss: 0.5040 - val_accuracy: 0.7895\n",
            "Epoch 85/500\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.4757 - accuracy: 0.8437 - val_loss: 0.5042 - val_accuracy: 0.7895\n",
            "Epoch 86/500\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.4701 - accuracy: 0.8510 - val_loss: 0.5041 - val_accuracy: 0.7895\n",
            "Epoch 87/500\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.4718 - accuracy: 0.8315 - val_loss: 0.5033 - val_accuracy: 0.7895\n",
            "Epoch 88/500\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.4882 - accuracy: 0.8235 - val_loss: 0.5033 - val_accuracy: 0.7895\n",
            "Epoch 89/500\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.5155 - accuracy: 0.7953 - val_loss: 0.5026 - val_accuracy: 0.7895\n",
            "Epoch 90/500\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.4520 - accuracy: 0.8439 - val_loss: 0.5031 - val_accuracy: 0.7895\n",
            "Epoch 91/500\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.4905 - accuracy: 0.8404 - val_loss: 0.5028 - val_accuracy: 0.7895\n",
            "Epoch 92/500\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.4872 - accuracy: 0.8218 - val_loss: 0.5029 - val_accuracy: 0.7895\n",
            "Epoch 93/500\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.4474 - accuracy: 0.8712 - val_loss: 0.5022 - val_accuracy: 0.7895\n",
            "Epoch 94/500\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.4685 - accuracy: 0.8417 - val_loss: 0.5012 - val_accuracy: 0.7895\n",
            "Epoch 95/500\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.4658 - accuracy: 0.8500 - val_loss: 0.5004 - val_accuracy: 0.8158\n",
            "Epoch 96/500\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.4590 - accuracy: 0.8477 - val_loss: 0.4998 - val_accuracy: 0.8158\n",
            "Epoch 97/500\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.4794 - accuracy: 0.8302 - val_loss: 0.4995 - val_accuracy: 0.8158\n",
            "Epoch 98/500\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.4954 - accuracy: 0.8107 - val_loss: 0.4999 - val_accuracy: 0.8158\n",
            "Epoch 99/500\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.4924 - accuracy: 0.8166 - val_loss: 0.4992 - val_accuracy: 0.8158\n",
            "Epoch 100/500\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.4735 - accuracy: 0.8379 - val_loss: 0.4990 - val_accuracy: 0.8158\n",
            "Epoch 101/500\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.4955 - accuracy: 0.8131 - val_loss: 0.4984 - val_accuracy: 0.8158\n",
            "Epoch 102/500\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.4367 - accuracy: 0.8621 - val_loss: 0.4984 - val_accuracy: 0.8158\n",
            "Epoch 103/500\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.4701 - accuracy: 0.8118 - val_loss: 0.4989 - val_accuracy: 0.8158\n",
            "Epoch 104/500\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.4545 - accuracy: 0.8511 - val_loss: 0.4987 - val_accuracy: 0.8158\n",
            "Epoch 105/500\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.4468 - accuracy: 0.8660 - val_loss: 0.4982 - val_accuracy: 0.8158\n",
            "Epoch 106/500\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.4753 - accuracy: 0.8190 - val_loss: 0.4987 - val_accuracy: 0.8158\n",
            "Epoch 107/500\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.4736 - accuracy: 0.8215 - val_loss: 0.4984 - val_accuracy: 0.8158\n",
            "Epoch 108/500\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.4626 - accuracy: 0.8353 - val_loss: 0.4982 - val_accuracy: 0.8158\n",
            "Epoch 109/500\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.4800 - accuracy: 0.8191 - val_loss: 0.4978 - val_accuracy: 0.8158\n",
            "Epoch 110/500\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.4577 - accuracy: 0.8413 - val_loss: 0.4971 - val_accuracy: 0.8158\n",
            "Epoch 111/500\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.4570 - accuracy: 0.8358 - val_loss: 0.4967 - val_accuracy: 0.8158\n",
            "Epoch 112/500\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.4583 - accuracy: 0.8456 - val_loss: 0.4957 - val_accuracy: 0.8158\n",
            "Epoch 113/500\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.4895 - accuracy: 0.8265 - val_loss: 0.4952 - val_accuracy: 0.8158\n",
            "Epoch 114/500\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.4697 - accuracy: 0.8382 - val_loss: 0.4948 - val_accuracy: 0.8158\n",
            "Epoch 115/500\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.4570 - accuracy: 0.8475 - val_loss: 0.4951 - val_accuracy: 0.8158\n",
            "Epoch 116/500\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.4656 - accuracy: 0.8281 - val_loss: 0.4952 - val_accuracy: 0.8158\n",
            "Epoch 117/500\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.4761 - accuracy: 0.8235 - val_loss: 0.4953 - val_accuracy: 0.8158\n",
            "Epoch 118/500\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.4659 - accuracy: 0.8323 - val_loss: 0.4959 - val_accuracy: 0.8421\n",
            "Epoch 119/500\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.4754 - accuracy: 0.8271 - val_loss: 0.4954 - val_accuracy: 0.8421\n",
            "Epoch 120/500\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.4732 - accuracy: 0.8401 - val_loss: 0.4952 - val_accuracy: 0.8421\n",
            "Epoch 121/500\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.4334 - accuracy: 0.8605 - val_loss: 0.4952 - val_accuracy: 0.8421\n",
            "Epoch 122/500\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.4800 - accuracy: 0.8421 - val_loss: 0.4940 - val_accuracy: 0.8158\n",
            "Epoch 123/500\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.5071 - accuracy: 0.7986 - val_loss: 0.4937 - val_accuracy: 0.8158\n",
            "Epoch 124/500\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.4611 - accuracy: 0.8500 - val_loss: 0.4938 - val_accuracy: 0.8158\n",
            "Epoch 125/500\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.4463 - accuracy: 0.8544 - val_loss: 0.4948 - val_accuracy: 0.8421\n",
            "Epoch 126/500\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.4835 - accuracy: 0.8054 - val_loss: 0.4953 - val_accuracy: 0.8421\n",
            "Epoch 127/500\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.4599 - accuracy: 0.8595 - val_loss: 0.4951 - val_accuracy: 0.8421\n",
            "Epoch 128/500\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.4585 - accuracy: 0.8337 - val_loss: 0.4945 - val_accuracy: 0.8421\n",
            "Epoch 129/500\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.4720 - accuracy: 0.8251 - val_loss: 0.4937 - val_accuracy: 0.8421\n",
            "Epoch 130/500\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.4360 - accuracy: 0.8663 - val_loss: 0.4929 - val_accuracy: 0.8158\n",
            "Epoch 131/500\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.4387 - accuracy: 0.8781 - val_loss: 0.4929 - val_accuracy: 0.8421\n",
            "Epoch 132/500\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.4600 - accuracy: 0.8487 - val_loss: 0.4928 - val_accuracy: 0.8421\n",
            "Epoch 133/500\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.5008 - accuracy: 0.8178 - val_loss: 0.4933 - val_accuracy: 0.8421\n",
            "Epoch 134/500\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.4593 - accuracy: 0.8275 - val_loss: 0.4932 - val_accuracy: 0.8421\n",
            "Epoch 135/500\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.4609 - accuracy: 0.8466 - val_loss: 0.4923 - val_accuracy: 0.8421\n",
            "Epoch 136/500\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.4733 - accuracy: 0.8212 - val_loss: 0.4925 - val_accuracy: 0.8421\n",
            "Epoch 137/500\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.4730 - accuracy: 0.8267 - val_loss: 0.4922 - val_accuracy: 0.8421\n",
            "Epoch 138/500\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.4376 - accuracy: 0.8573 - val_loss: 0.4918 - val_accuracy: 0.8421\n",
            "Epoch 139/500\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.4715 - accuracy: 0.8163 - val_loss: 0.4919 - val_accuracy: 0.8421\n",
            "Epoch 140/500\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.4445 - accuracy: 0.8621 - val_loss: 0.4918 - val_accuracy: 0.8421\n",
            "Epoch 141/500\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.4287 - accuracy: 0.8610 - val_loss: 0.4925 - val_accuracy: 0.8421\n",
            "Epoch 142/500\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.4813 - accuracy: 0.8217 - val_loss: 0.4928 - val_accuracy: 0.8684\n",
            "Epoch 143/500\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.4627 - accuracy: 0.8207 - val_loss: 0.4928 - val_accuracy: 0.8684\n",
            "Epoch 144/500\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.4644 - accuracy: 0.8362 - val_loss: 0.4925 - val_accuracy: 0.8421\n",
            "Epoch 145/500\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.4354 - accuracy: 0.8512 - val_loss: 0.4925 - val_accuracy: 0.8421\n",
            "Epoch 146/500\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.4568 - accuracy: 0.8353 - val_loss: 0.4933 - val_accuracy: 0.8684\n",
            "Epoch 147/500\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.4346 - accuracy: 0.8600 - val_loss: 0.4930 - val_accuracy: 0.8684\n",
            "Epoch 148/500\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.4617 - accuracy: 0.8376 - val_loss: 0.4934 - val_accuracy: 0.8684\n",
            "Epoch 149/500\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.4716 - accuracy: 0.8444 - val_loss: 0.4931 - val_accuracy: 0.8684\n",
            "Epoch 150/500\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.4592 - accuracy: 0.8316 - val_loss: 0.4932 - val_accuracy: 0.8684\n",
            "Epoch 151/500\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.4513 - accuracy: 0.8562 - val_loss: 0.4920 - val_accuracy: 0.8684\n",
            "Epoch 152/500\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.4476 - accuracy: 0.8528 - val_loss: 0.4930 - val_accuracy: 0.8684\n",
            "Epoch 153/500\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.4768 - accuracy: 0.8167 - val_loss: 0.4922 - val_accuracy: 0.8684\n",
            "Epoch 154/500\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.4557 - accuracy: 0.8275 - val_loss: 0.4921 - val_accuracy: 0.8684\n",
            "Epoch 155/500\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.4449 - accuracy: 0.8512 - val_loss: 0.4926 - val_accuracy: 0.8684\n",
            "Epoch 156/500\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.4862 - accuracy: 0.7900 - val_loss: 0.4932 - val_accuracy: 0.8684\n",
            "Epoch 157/500\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.4592 - accuracy: 0.8444 - val_loss: 0.4934 - val_accuracy: 0.8684\n",
            "Epoch 158/500\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.5086 - accuracy: 0.7884 - val_loss: 0.4927 - val_accuracy: 0.8684\n",
            "Epoch 159/500\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.4485 - accuracy: 0.8468 - val_loss: 0.4926 - val_accuracy: 0.8684\n",
            "Epoch 160/500\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.4620 - accuracy: 0.8233 - val_loss: 0.4925 - val_accuracy: 0.8684\n",
            "Epoch 161/500\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.4508 - accuracy: 0.8480 - val_loss: 0.4921 - val_accuracy: 0.8684\n",
            "Epoch 162/500\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.4554 - accuracy: 0.8306 - val_loss: 0.4908 - val_accuracy: 0.8421\n",
            "Epoch 163/500\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.4630 - accuracy: 0.8474 - val_loss: 0.4908 - val_accuracy: 0.8421\n",
            "Epoch 164/500\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.4782 - accuracy: 0.8168 - val_loss: 0.4903 - val_accuracy: 0.8421\n",
            "Epoch 165/500\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.4572 - accuracy: 0.8411 - val_loss: 0.4902 - val_accuracy: 0.8421\n",
            "Epoch 166/500\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.4630 - accuracy: 0.8280 - val_loss: 0.4901 - val_accuracy: 0.8421\n",
            "Epoch 167/500\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.4584 - accuracy: 0.8505 - val_loss: 0.4893 - val_accuracy: 0.8421\n",
            "Epoch 168/500\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.4461 - accuracy: 0.8331 - val_loss: 0.4901 - val_accuracy: 0.8421\n",
            "Epoch 169/500\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.4311 - accuracy: 0.8472 - val_loss: 0.4908 - val_accuracy: 0.8684\n",
            "Epoch 170/500\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.4696 - accuracy: 0.8226 - val_loss: 0.4918 - val_accuracy: 0.8684\n",
            "Epoch 171/500\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.4594 - accuracy: 0.8385 - val_loss: 0.4916 - val_accuracy: 0.8684\n",
            "Epoch 172/500\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.4739 - accuracy: 0.8303 - val_loss: 0.4916 - val_accuracy: 0.8684\n",
            "Epoch 173/500\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.4614 - accuracy: 0.8269 - val_loss: 0.4917 - val_accuracy: 0.8684\n",
            "Epoch 174/500\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.4428 - accuracy: 0.8449 - val_loss: 0.4915 - val_accuracy: 0.8684\n",
            "Epoch 175/500\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.4654 - accuracy: 0.8344 - val_loss: 0.4909 - val_accuracy: 0.8684\n",
            "Epoch 176/500\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.4326 - accuracy: 0.8686 - val_loss: 0.4904 - val_accuracy: 0.8684\n",
            "Epoch 177/500\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.4478 - accuracy: 0.8472 - val_loss: 0.4901 - val_accuracy: 0.8684\n",
            "Epoch 178/500\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.4502 - accuracy: 0.8249 - val_loss: 0.4901 - val_accuracy: 0.8684\n",
            "Epoch 179/500\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.4533 - accuracy: 0.8289 - val_loss: 0.4912 - val_accuracy: 0.8684\n",
            "Epoch 180/500\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.4782 - accuracy: 0.8392 - val_loss: 0.4915 - val_accuracy: 0.8684\n",
            "Epoch 181/500\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.4368 - accuracy: 0.8632 - val_loss: 0.4919 - val_accuracy: 0.8684\n",
            "Epoch 182/500\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.4800 - accuracy: 0.8276 - val_loss: 0.4917 - val_accuracy: 0.8684\n",
            "Epoch 183/500\n",
            "15/15 [==============================] - 0s 6ms/step - loss: 0.4858 - accuracy: 0.7903 - val_loss: 0.4916 - val_accuracy: 0.8684\n",
            "Epoch 184/500\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.4497 - accuracy: 0.8505 - val_loss: 0.4916 - val_accuracy: 0.8684\n",
            "Epoch 185/500\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.4463 - accuracy: 0.8548 - val_loss: 0.4917 - val_accuracy: 0.8684\n",
            "Epoch 186/500\n",
            "15/15 [==============================] - 0s 6ms/step - loss: 0.4925 - accuracy: 0.7909 - val_loss: 0.4915 - val_accuracy: 0.8684\n",
            "Epoch 187/500\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.4595 - accuracy: 0.8384 - val_loss: 0.4915 - val_accuracy: 0.8684\n",
            "Epoch 188/500\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.4785 - accuracy: 0.7990 - val_loss: 0.4901 - val_accuracy: 0.8684\n",
            "Epoch 189/500\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.4511 - accuracy: 0.8410 - val_loss: 0.4896 - val_accuracy: 0.8421\n",
            "Epoch 190/500\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.4925 - accuracy: 0.8273 - val_loss: 0.4894 - val_accuracy: 0.8421\n",
            "Epoch 191/500\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.4279 - accuracy: 0.8693 - val_loss: 0.4885 - val_accuracy: 0.8421\n",
            "Epoch 192/500\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.4470 - accuracy: 0.8381 - val_loss: 0.4881 - val_accuracy: 0.8421\n",
            "Epoch 193/500\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.4369 - accuracy: 0.8677 - val_loss: 0.4893 - val_accuracy: 0.8684\n",
            "Epoch 194/500\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.4773 - accuracy: 0.8116 - val_loss: 0.4895 - val_accuracy: 0.8684\n",
            "Epoch 195/500\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.4717 - accuracy: 0.8106 - val_loss: 0.4886 - val_accuracy: 0.8421\n",
            "Epoch 196/500\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.4964 - accuracy: 0.8101 - val_loss: 0.4897 - val_accuracy: 0.8684\n",
            "Epoch 197/500\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.4647 - accuracy: 0.8134 - val_loss: 0.4894 - val_accuracy: 0.8684\n",
            "Epoch 198/500\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.4706 - accuracy: 0.8372 - val_loss: 0.4885 - val_accuracy: 0.8421\n",
            "Epoch 199/500\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.4949 - accuracy: 0.8126 - val_loss: 0.4886 - val_accuracy: 0.8684\n",
            "Epoch 200/500\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.4479 - accuracy: 0.8589 - val_loss: 0.4889 - val_accuracy: 0.8684\n",
            "Epoch 201/500\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.4608 - accuracy: 0.8351 - val_loss: 0.4901 - val_accuracy: 0.8684\n",
            "Epoch 202/500\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.4721 - accuracy: 0.8079 - val_loss: 0.4903 - val_accuracy: 0.8684\n",
            "Epoch 203/500\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.4417 - accuracy: 0.8359 - val_loss: 0.4898 - val_accuracy: 0.8684\n",
            "Epoch 204/500\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.4693 - accuracy: 0.8241 - val_loss: 0.4898 - val_accuracy: 0.8684\n",
            "Epoch 205/500\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.4854 - accuracy: 0.8187 - val_loss: 0.4889 - val_accuracy: 0.8684\n",
            "Epoch 206/500\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.4120 - accuracy: 0.8751 - val_loss: 0.4891 - val_accuracy: 0.8684\n",
            "Epoch 207/500\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.4408 - accuracy: 0.8361 - val_loss: 0.4904 - val_accuracy: 0.8684\n",
            "Epoch 208/500\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.4739 - accuracy: 0.8203 - val_loss: 0.4912 - val_accuracy: 0.8684\n",
            "Epoch 209/500\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.4573 - accuracy: 0.8292 - val_loss: 0.4911 - val_accuracy: 0.8684\n",
            "Epoch 210/500\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.4713 - accuracy: 0.8093 - val_loss: 0.4908 - val_accuracy: 0.8684\n",
            "Epoch 211/500\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.4504 - accuracy: 0.8500 - val_loss: 0.4909 - val_accuracy: 0.8684\n",
            "Epoch 212/500\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.4622 - accuracy: 0.8416 - val_loss: 0.4907 - val_accuracy: 0.8684\n",
            "Epoch 213/500\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.4747 - accuracy: 0.8374 - val_loss: 0.4902 - val_accuracy: 0.8684\n",
            "Epoch 214/500\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.4700 - accuracy: 0.8363 - val_loss: 0.4915 - val_accuracy: 0.8684\n",
            "Epoch 215/500\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.4396 - accuracy: 0.8432 - val_loss: 0.4914 - val_accuracy: 0.8684\n",
            "Epoch 216/500\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.4806 - accuracy: 0.8231 - val_loss: 0.4909 - val_accuracy: 0.8684\n",
            "Epoch 217/500\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.4574 - accuracy: 0.8341 - val_loss: 0.4912 - val_accuracy: 0.8684\n",
            "Epoch 218/500\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.4503 - accuracy: 0.8458 - val_loss: 0.4907 - val_accuracy: 0.8684\n",
            "Epoch 219/500\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.4421 - accuracy: 0.8615 - val_loss: 0.4912 - val_accuracy: 0.8684\n",
            "Epoch 220/500\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.4511 - accuracy: 0.8544 - val_loss: 0.4916 - val_accuracy: 0.8684\n",
            "Epoch 221/500\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.4399 - accuracy: 0.8499 - val_loss: 0.4913 - val_accuracy: 0.8684\n",
            "Epoch 222/500\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.4413 - accuracy: 0.8534 - val_loss: 0.4904 - val_accuracy: 0.8684\n",
            "Epoch 223/500\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.4524 - accuracy: 0.8593 - val_loss: 0.4890 - val_accuracy: 0.8684\n",
            "Epoch 224/500\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.4523 - accuracy: 0.8296 - val_loss: 0.4894 - val_accuracy: 0.8684\n",
            "Epoch 225/500\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.4374 - accuracy: 0.8541 - val_loss: 0.4901 - val_accuracy: 0.8684\n",
            "Epoch 226/500\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.4376 - accuracy: 0.8606 - val_loss: 0.4899 - val_accuracy: 0.8684\n",
            "Epoch 227/500\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.4688 - accuracy: 0.8212 - val_loss: 0.4909 - val_accuracy: 0.8684\n",
            "Epoch 228/500\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.4749 - accuracy: 0.8537 - val_loss: 0.4900 - val_accuracy: 0.8684\n",
            "Epoch 229/500\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.4425 - accuracy: 0.8421 - val_loss: 0.4899 - val_accuracy: 0.8684\n",
            "Epoch 230/500\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.4461 - accuracy: 0.8464 - val_loss: 0.4905 - val_accuracy: 0.8684\n",
            "Epoch 231/500\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.4575 - accuracy: 0.8438 - val_loss: 0.4905 - val_accuracy: 0.8684\n",
            "Epoch 232/500\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.5140 - accuracy: 0.7992 - val_loss: 0.4898 - val_accuracy: 0.8684\n",
            "Epoch 233/500\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.4292 - accuracy: 0.8568 - val_loss: 0.4898 - val_accuracy: 0.8684\n",
            "Epoch 234/500\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.4454 - accuracy: 0.8599 - val_loss: 0.4903 - val_accuracy: 0.8684\n",
            "Epoch 235/500\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.4548 - accuracy: 0.8344 - val_loss: 0.4898 - val_accuracy: 0.8684\n",
            "Epoch 236/500\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.4361 - accuracy: 0.8390 - val_loss: 0.4890 - val_accuracy: 0.8684\n",
            "Epoch 237/500\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.4421 - accuracy: 0.8469 - val_loss: 0.4887 - val_accuracy: 0.8421\n",
            "Epoch 238/500\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.4491 - accuracy: 0.8310 - val_loss: 0.4894 - val_accuracy: 0.8684\n",
            "Epoch 239/500\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.4487 - accuracy: 0.8505 - val_loss: 0.4906 - val_accuracy: 0.8684\n",
            "Epoch 240/500\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.4792 - accuracy: 0.8313 - val_loss: 0.4901 - val_accuracy: 0.8684\n",
            "Epoch 241/500\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.4465 - accuracy: 0.8379 - val_loss: 0.4894 - val_accuracy: 0.8684\n",
            "Epoch 242/500\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.4741 - accuracy: 0.8231 - val_loss: 0.4901 - val_accuracy: 0.8684\n",
            "Epoch 243/500\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.4674 - accuracy: 0.8325 - val_loss: 0.4891 - val_accuracy: 0.8684\n",
            "Epoch 244/500\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.4493 - accuracy: 0.8446 - val_loss: 0.4887 - val_accuracy: 0.8684\n",
            "Epoch 245/500\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.4768 - accuracy: 0.8168 - val_loss: 0.4888 - val_accuracy: 0.8684\n",
            "Epoch 246/500\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.4237 - accuracy: 0.8638 - val_loss: 0.4881 - val_accuracy: 0.8421\n",
            "Epoch 247/500\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.4545 - accuracy: 0.8566 - val_loss: 0.4873 - val_accuracy: 0.8421\n",
            "Epoch 248/500\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.4499 - accuracy: 0.8620 - val_loss: 0.4867 - val_accuracy: 0.8421\n",
            "Epoch 249/500\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.4662 - accuracy: 0.8245 - val_loss: 0.4862 - val_accuracy: 0.8421\n",
            "Epoch 250/500\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.4325 - accuracy: 0.8624 - val_loss: 0.4867 - val_accuracy: 0.8421\n",
            "Epoch 251/500\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.4426 - accuracy: 0.8440 - val_loss: 0.4883 - val_accuracy: 0.8684\n",
            "Epoch 252/500\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.4411 - accuracy: 0.8550 - val_loss: 0.4883 - val_accuracy: 0.8684\n",
            "Epoch 253/500\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.4214 - accuracy: 0.8711 - val_loss: 0.4887 - val_accuracy: 0.8684\n",
            "Epoch 254/500\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.4362 - accuracy: 0.8729 - val_loss: 0.4892 - val_accuracy: 0.8684\n",
            "Epoch 255/500\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.4893 - accuracy: 0.7851 - val_loss: 0.4898 - val_accuracy: 0.8684\n",
            "Epoch 256/500\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.4374 - accuracy: 0.8673 - val_loss: 0.4899 - val_accuracy: 0.8684\n",
            "Epoch 257/500\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.4552 - accuracy: 0.8425 - val_loss: 0.4905 - val_accuracy: 0.8684\n",
            "Epoch 258/500\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.4603 - accuracy: 0.8387 - val_loss: 0.4912 - val_accuracy: 0.8684\n",
            "Epoch 259/500\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.4328 - accuracy: 0.8485 - val_loss: 0.4916 - val_accuracy: 0.8684\n",
            "Epoch 260/500\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.4606 - accuracy: 0.8553 - val_loss: 0.4907 - val_accuracy: 0.8684\n",
            "Epoch 261/500\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.4726 - accuracy: 0.8163 - val_loss: 0.4909 - val_accuracy: 0.8684\n",
            "Epoch 262/500\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.4406 - accuracy: 0.8356 - val_loss: 0.4901 - val_accuracy: 0.8684\n",
            "Epoch 263/500\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.4871 - accuracy: 0.8339 - val_loss: 0.4895 - val_accuracy: 0.8684\n",
            "Epoch 264/500\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.4437 - accuracy: 0.8408 - val_loss: 0.4888 - val_accuracy: 0.8684\n",
            "Epoch 265/500\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.4365 - accuracy: 0.8514 - val_loss: 0.4881 - val_accuracy: 0.8684\n",
            "Epoch 266/500\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.4314 - accuracy: 0.8848 - val_loss: 0.4881 - val_accuracy: 0.8684\n",
            "Epoch 267/500\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.4491 - accuracy: 0.8501 - val_loss: 0.4877 - val_accuracy: 0.8421\n",
            "Epoch 268/500\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.4671 - accuracy: 0.8372 - val_loss: 0.4880 - val_accuracy: 0.8421\n",
            "Epoch 269/500\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.4459 - accuracy: 0.8582 - val_loss: 0.4884 - val_accuracy: 0.8684\n",
            "Epoch 270/500\n",
            "15/15 [==============================] - 0s 6ms/step - loss: 0.4540 - accuracy: 0.8345 - val_loss: 0.4887 - val_accuracy: 0.8684\n",
            "Epoch 271/500\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.4339 - accuracy: 0.8716 - val_loss: 0.4882 - val_accuracy: 0.8421\n",
            "Epoch 272/500\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.4655 - accuracy: 0.8314 - val_loss: 0.4888 - val_accuracy: 0.8684\n",
            "Epoch 273/500\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.4733 - accuracy: 0.8381 - val_loss: 0.4884 - val_accuracy: 0.8684\n",
            "Epoch 274/500\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.4522 - accuracy: 0.8478 - val_loss: 0.4891 - val_accuracy: 0.8684\n",
            "Epoch 275/500\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.4295 - accuracy: 0.8779 - val_loss: 0.4901 - val_accuracy: 0.8684\n",
            "Epoch 276/500\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.4252 - accuracy: 0.8707 - val_loss: 0.4897 - val_accuracy: 0.8684\n",
            "Epoch 277/500\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.4425 - accuracy: 0.8679 - val_loss: 0.4902 - val_accuracy: 0.8684\n",
            "Epoch 278/500\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.4600 - accuracy: 0.8524 - val_loss: 0.4904 - val_accuracy: 0.8684\n",
            "Epoch 279/500\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.4445 - accuracy: 0.8342 - val_loss: 0.4907 - val_accuracy: 0.8684\n",
            "Epoch 280/500\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.4812 - accuracy: 0.8240 - val_loss: 0.4905 - val_accuracy: 0.8684\n",
            "Epoch 281/500\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.4886 - accuracy: 0.8117 - val_loss: 0.4901 - val_accuracy: 0.8684\n",
            "Epoch 282/500\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.4537 - accuracy: 0.8565 - val_loss: 0.4899 - val_accuracy: 0.8684\n",
            "Epoch 283/500\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.4654 - accuracy: 0.8698 - val_loss: 0.4891 - val_accuracy: 0.8684\n",
            "Epoch 284/500\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.4500 - accuracy: 0.8621 - val_loss: 0.4889 - val_accuracy: 0.8684\n",
            "Epoch 285/500\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.4487 - accuracy: 0.8500 - val_loss: 0.4892 - val_accuracy: 0.8684\n",
            "Epoch 286/500\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.4162 - accuracy: 0.8849 - val_loss: 0.4898 - val_accuracy: 0.8684\n",
            "Epoch 287/500\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.4482 - accuracy: 0.8625 - val_loss: 0.4897 - val_accuracy: 0.8684\n",
            "Epoch 288/500\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.4648 - accuracy: 0.8332 - val_loss: 0.4894 - val_accuracy: 0.8684\n",
            "Epoch 289/500\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.4753 - accuracy: 0.8343 - val_loss: 0.4894 - val_accuracy: 0.8684\n",
            "Epoch 290/500\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.4817 - accuracy: 0.8271 - val_loss: 0.4895 - val_accuracy: 0.8684\n",
            "Epoch 291/500\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.4438 - accuracy: 0.8638 - val_loss: 0.4896 - val_accuracy: 0.8684\n",
            "Epoch 292/500\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.4361 - accuracy: 0.8623 - val_loss: 0.4898 - val_accuracy: 0.8684\n",
            "Epoch 293/500\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.4639 - accuracy: 0.8563 - val_loss: 0.4890 - val_accuracy: 0.8684\n",
            "Epoch 294/500\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.4689 - accuracy: 0.8334 - val_loss: 0.4884 - val_accuracy: 0.8684\n",
            "Epoch 295/500\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.4390 - accuracy: 0.8480 - val_loss: 0.4888 - val_accuracy: 0.8684\n",
            "Epoch 296/500\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.4472 - accuracy: 0.8463 - val_loss: 0.4883 - val_accuracy: 0.8684\n",
            "Epoch 297/500\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.4377 - accuracy: 0.8558 - val_loss: 0.4879 - val_accuracy: 0.8421\n",
            "Epoch 298/500\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.4986 - accuracy: 0.8109 - val_loss: 0.4886 - val_accuracy: 0.8684\n",
            "Epoch 299/500\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.4582 - accuracy: 0.8456 - val_loss: 0.4884 - val_accuracy: 0.8684\n",
            "Epoch 300/500\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.4459 - accuracy: 0.8390 - val_loss: 0.4893 - val_accuracy: 0.8684\n",
            "Epoch 301/500\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.4988 - accuracy: 0.8319 - val_loss: 0.4896 - val_accuracy: 0.8684\n",
            "Epoch 302/500\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.4297 - accuracy: 0.8794 - val_loss: 0.4883 - val_accuracy: 0.8684\n",
            "Epoch 303/500\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.4232 - accuracy: 0.8877 - val_loss: 0.4882 - val_accuracy: 0.8684\n",
            "Epoch 304/500\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.4039 - accuracy: 0.8969 - val_loss: 0.4884 - val_accuracy: 0.8684\n",
            "Epoch 305/500\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.4497 - accuracy: 0.8510 - val_loss: 0.4891 - val_accuracy: 0.8684\n",
            "Epoch 306/500\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.4360 - accuracy: 0.8473 - val_loss: 0.4892 - val_accuracy: 0.8684\n",
            "Epoch 307/500\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.4673 - accuracy: 0.8375 - val_loss: 0.4892 - val_accuracy: 0.8684\n",
            "Epoch 308/500\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.4457 - accuracy: 0.8647 - val_loss: 0.4887 - val_accuracy: 0.8421\n",
            "Epoch 309/500\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.4661 - accuracy: 0.8460 - val_loss: 0.4889 - val_accuracy: 0.8684\n",
            "Epoch 310/500\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.4369 - accuracy: 0.8582 - val_loss: 0.4884 - val_accuracy: 0.8421\n",
            "Epoch 311/500\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.4576 - accuracy: 0.8428 - val_loss: 0.4883 - val_accuracy: 0.8421\n",
            "Epoch 312/500\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.4435 - accuracy: 0.8679 - val_loss: 0.4893 - val_accuracy: 0.8684\n",
            "Epoch 313/500\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.4202 - accuracy: 0.8959 - val_loss: 0.4894 - val_accuracy: 0.8684\n",
            "Epoch 314/500\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.4528 - accuracy: 0.8771 - val_loss: 0.4898 - val_accuracy: 0.8684\n",
            "Epoch 315/500\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.4393 - accuracy: 0.8773 - val_loss: 0.4903 - val_accuracy: 0.8684\n",
            "Epoch 316/500\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.4101 - accuracy: 0.8831 - val_loss: 0.4897 - val_accuracy: 0.8684\n",
            "Epoch 317/500\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.4443 - accuracy: 0.8690 - val_loss: 0.4906 - val_accuracy: 0.8684\n",
            "Epoch 318/500\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.4509 - accuracy: 0.8563 - val_loss: 0.4915 - val_accuracy: 0.8684\n",
            "Epoch 319/500\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.4449 - accuracy: 0.8486 - val_loss: 0.4910 - val_accuracy: 0.8684\n",
            "Epoch 320/500\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.4346 - accuracy: 0.8708 - val_loss: 0.4911 - val_accuracy: 0.8684\n",
            "Epoch 321/500\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.4377 - accuracy: 0.8432 - val_loss: 0.4915 - val_accuracy: 0.8684\n",
            "Epoch 322/500\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.4415 - accuracy: 0.8452 - val_loss: 0.4914 - val_accuracy: 0.8684\n",
            "Epoch 323/500\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.4569 - accuracy: 0.8654 - val_loss: 0.4910 - val_accuracy: 0.8684\n",
            "Epoch 324/500\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.4501 - accuracy: 0.8331 - val_loss: 0.4914 - val_accuracy: 0.8684\n",
            "Epoch 325/500\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.4408 - accuracy: 0.8507 - val_loss: 0.4917 - val_accuracy: 0.8684\n",
            "Epoch 326/500\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.4806 - accuracy: 0.8262 - val_loss: 0.4911 - val_accuracy: 0.8684\n",
            "Epoch 327/500\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.4514 - accuracy: 0.8663 - val_loss: 0.4905 - val_accuracy: 0.8684\n",
            "Epoch 328/500\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.4446 - accuracy: 0.8524 - val_loss: 0.4909 - val_accuracy: 0.8684\n",
            "Epoch 329/500\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.4427 - accuracy: 0.8511 - val_loss: 0.4909 - val_accuracy: 0.8684\n",
            "Epoch 330/500\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.4426 - accuracy: 0.8455 - val_loss: 0.4898 - val_accuracy: 0.8421\n",
            "Epoch 331/500\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.4739 - accuracy: 0.8505 - val_loss: 0.4896 - val_accuracy: 0.8421\n",
            "Epoch 332/500\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.4760 - accuracy: 0.8491 - val_loss: 0.4902 - val_accuracy: 0.8421\n",
            "Epoch 333/500\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.4678 - accuracy: 0.8388 - val_loss: 0.4897 - val_accuracy: 0.8421\n",
            "Epoch 334/500\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.4764 - accuracy: 0.8446 - val_loss: 0.4891 - val_accuracy: 0.8421\n",
            "Epoch 335/500\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.4856 - accuracy: 0.8432 - val_loss: 0.4891 - val_accuracy: 0.8421\n",
            "Epoch 336/500\n",
            "15/15 [==============================] - 0s 6ms/step - loss: 0.4647 - accuracy: 0.8470 - val_loss: 0.4904 - val_accuracy: 0.8421\n",
            "Epoch 337/500\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.4528 - accuracy: 0.8674 - val_loss: 0.4903 - val_accuracy: 0.8421\n",
            "Epoch 338/500\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.4645 - accuracy: 0.8451 - val_loss: 0.4901 - val_accuracy: 0.8684\n",
            "Epoch 339/500\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.4242 - accuracy: 0.8760 - val_loss: 0.4899 - val_accuracy: 0.8421\n",
            "Epoch 340/500\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.5018 - accuracy: 0.8171 - val_loss: 0.4903 - val_accuracy: 0.8684\n",
            "Epoch 341/500\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.4197 - accuracy: 0.8917 - val_loss: 0.4899 - val_accuracy: 0.8421\n",
            "Epoch 342/500\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.4315 - accuracy: 0.8724 - val_loss: 0.4902 - val_accuracy: 0.8421\n",
            "Epoch 343/500\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.4674 - accuracy: 0.8469 - val_loss: 0.4903 - val_accuracy: 0.8421\n",
            "Epoch 344/500\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.4517 - accuracy: 0.8512 - val_loss: 0.4911 - val_accuracy: 0.8684\n",
            "Epoch 345/500\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.4306 - accuracy: 0.8749 - val_loss: 0.4907 - val_accuracy: 0.8421\n",
            "Epoch 346/500\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.4634 - accuracy: 0.8332 - val_loss: 0.4905 - val_accuracy: 0.8421\n",
            "Epoch 347/500\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.4375 - accuracy: 0.8497 - val_loss: 0.4918 - val_accuracy: 0.8684\n",
            "Epoch 348/500\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.4421 - accuracy: 0.8738 - val_loss: 0.4929 - val_accuracy: 0.8684\n",
            "Epoch 349/500\n",
            "15/15 [==============================] - 0s 6ms/step - loss: 0.4465 - accuracy: 0.8488 - val_loss: 0.4926 - val_accuracy: 0.8684\n",
            "Epoch 350/500\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.4529 - accuracy: 0.8591 - val_loss: 0.4919 - val_accuracy: 0.8684\n",
            "Epoch 351/500\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.4666 - accuracy: 0.8555 - val_loss: 0.4908 - val_accuracy: 0.8421\n",
            "Epoch 352/500\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.4841 - accuracy: 0.8396 - val_loss: 0.4896 - val_accuracy: 0.8421\n",
            "Epoch 353/500\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.4479 - accuracy: 0.8648 - val_loss: 0.4893 - val_accuracy: 0.8421\n",
            "Epoch 354/500\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.4266 - accuracy: 0.8839 - val_loss: 0.4895 - val_accuracy: 0.8421\n",
            "Epoch 355/500\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.4441 - accuracy: 0.8558 - val_loss: 0.4899 - val_accuracy: 0.8421\n",
            "Epoch 356/500\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.4797 - accuracy: 0.8603 - val_loss: 0.4903 - val_accuracy: 0.8421\n",
            "Epoch 357/500\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.4836 - accuracy: 0.8407 - val_loss: 0.4905 - val_accuracy: 0.8421\n",
            "Epoch 358/500\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.4879 - accuracy: 0.8360 - val_loss: 0.4900 - val_accuracy: 0.8421\n",
            "Epoch 359/500\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.4618 - accuracy: 0.8449 - val_loss: 0.4896 - val_accuracy: 0.8421\n",
            "Epoch 360/500\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.4449 - accuracy: 0.8596 - val_loss: 0.4902 - val_accuracy: 0.8421\n",
            "Epoch 361/500\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.4389 - accuracy: 0.8817 - val_loss: 0.4905 - val_accuracy: 0.8684\n",
            "Epoch 362/500\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.4323 - accuracy: 0.8711 - val_loss: 0.4915 - val_accuracy: 0.8684\n",
            "Epoch 363/500\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.4480 - accuracy: 0.8678 - val_loss: 0.4923 - val_accuracy: 0.8684\n",
            "Epoch 364/500\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.4284 - accuracy: 0.8710 - val_loss: 0.4932 - val_accuracy: 0.8684\n",
            "Epoch 365/500\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.4228 - accuracy: 0.8763 - val_loss: 0.4926 - val_accuracy: 0.8684\n",
            "Epoch 366/500\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.4707 - accuracy: 0.8436 - val_loss: 0.4927 - val_accuracy: 0.8684\n",
            "Epoch 367/500\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.4420 - accuracy: 0.8518 - val_loss: 0.4923 - val_accuracy: 0.8684\n",
            "Epoch 368/500\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.4041 - accuracy: 0.8808 - val_loss: 0.4919 - val_accuracy: 0.8684\n",
            "Epoch 369/500\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.4288 - accuracy: 0.8802 - val_loss: 0.4910 - val_accuracy: 0.8421\n",
            "Epoch 370/500\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.4516 - accuracy: 0.8595 - val_loss: 0.4909 - val_accuracy: 0.8421\n",
            "Epoch 371/500\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.4163 - accuracy: 0.8872 - val_loss: 0.4912 - val_accuracy: 0.8421\n",
            "Epoch 372/500\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.4219 - accuracy: 0.8825 - val_loss: 0.4908 - val_accuracy: 0.8421\n",
            "Epoch 373/500\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.4272 - accuracy: 0.8716 - val_loss: 0.4899 - val_accuracy: 0.8421\n",
            "Epoch 374/500\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.4701 - accuracy: 0.8336 - val_loss: 0.4897 - val_accuracy: 0.8421\n",
            "Epoch 375/500\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.4622 - accuracy: 0.8415 - val_loss: 0.4897 - val_accuracy: 0.8421\n",
            "Epoch 376/500\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.4871 - accuracy: 0.8253 - val_loss: 0.4897 - val_accuracy: 0.8421\n",
            "Epoch 377/500\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.4627 - accuracy: 0.8712 - val_loss: 0.4904 - val_accuracy: 0.8421\n",
            "Epoch 378/500\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.4312 - accuracy: 0.8758 - val_loss: 0.4916 - val_accuracy: 0.8421\n",
            "Epoch 379/500\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.4553 - accuracy: 0.8651 - val_loss: 0.4920 - val_accuracy: 0.8421\n",
            "Epoch 380/500\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.4733 - accuracy: 0.8389 - val_loss: 0.4909 - val_accuracy: 0.8421\n",
            "Epoch 381/500\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.4640 - accuracy: 0.8418 - val_loss: 0.4915 - val_accuracy: 0.8421\n",
            "Epoch 382/500\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.4489 - accuracy: 0.8608 - val_loss: 0.4911 - val_accuracy: 0.8421\n",
            "Epoch 383/500\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.4422 - accuracy: 0.8536 - val_loss: 0.4904 - val_accuracy: 0.8421\n",
            "Epoch 384/500\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.4502 - accuracy: 0.8816 - val_loss: 0.4905 - val_accuracy: 0.8421\n",
            "Epoch 385/500\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.4507 - accuracy: 0.8490 - val_loss: 0.4914 - val_accuracy: 0.8421\n",
            "Epoch 386/500\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.4436 - accuracy: 0.8610 - val_loss: 0.4919 - val_accuracy: 0.8421\n",
            "Epoch 387/500\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.4444 - accuracy: 0.8563 - val_loss: 0.4912 - val_accuracy: 0.8421\n",
            "Epoch 388/500\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.4466 - accuracy: 0.8680 - val_loss: 0.4906 - val_accuracy: 0.8421\n",
            "Epoch 389/500\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.4593 - accuracy: 0.8410 - val_loss: 0.4905 - val_accuracy: 0.8421\n",
            "Epoch 390/500\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.4748 - accuracy: 0.8442 - val_loss: 0.4909 - val_accuracy: 0.8421\n",
            "Epoch 391/500\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.4244 - accuracy: 0.8784 - val_loss: 0.4914 - val_accuracy: 0.8684\n",
            "Epoch 392/500\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.4461 - accuracy: 0.8533 - val_loss: 0.4913 - val_accuracy: 0.8684\n",
            "Epoch 393/500\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.4618 - accuracy: 0.8336 - val_loss: 0.4911 - val_accuracy: 0.8684\n",
            "Epoch 394/500\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.4605 - accuracy: 0.8135 - val_loss: 0.4906 - val_accuracy: 0.8421\n",
            "Epoch 395/500\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.4326 - accuracy: 0.8822 - val_loss: 0.4904 - val_accuracy: 0.8421\n",
            "Epoch 396/500\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.4262 - accuracy: 0.8622 - val_loss: 0.4909 - val_accuracy: 0.8421\n",
            "Epoch 397/500\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.4460 - accuracy: 0.8696 - val_loss: 0.4900 - val_accuracy: 0.8421\n",
            "Epoch 398/500\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.4516 - accuracy: 0.8447 - val_loss: 0.4906 - val_accuracy: 0.8421\n",
            "Epoch 399/500\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.4609 - accuracy: 0.8457 - val_loss: 0.4915 - val_accuracy: 0.8421\n",
            "Epoch 400/500\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.4201 - accuracy: 0.8969 - val_loss: 0.4915 - val_accuracy: 0.8421\n",
            "Epoch 401/500\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.4677 - accuracy: 0.8520 - val_loss: 0.4914 - val_accuracy: 0.8421\n",
            "Epoch 402/500\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.4641 - accuracy: 0.8482 - val_loss: 0.4909 - val_accuracy: 0.8421\n",
            "Epoch 403/500\n",
            "15/15 [==============================] - 0s 6ms/step - loss: 0.4549 - accuracy: 0.8545 - val_loss: 0.4910 - val_accuracy: 0.8421\n",
            "Epoch 404/500\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.4555 - accuracy: 0.8535 - val_loss: 0.4903 - val_accuracy: 0.8421\n",
            "Epoch 405/500\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.4487 - accuracy: 0.8593 - val_loss: 0.4907 - val_accuracy: 0.8421\n",
            "Epoch 406/500\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.4525 - accuracy: 0.8457 - val_loss: 0.4915 - val_accuracy: 0.8421\n",
            "Epoch 407/500\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.4423 - accuracy: 0.8680 - val_loss: 0.4925 - val_accuracy: 0.8684\n",
            "Epoch 408/500\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.4327 - accuracy: 0.8631 - val_loss: 0.4925 - val_accuracy: 0.8684\n",
            "Epoch 409/500\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.4549 - accuracy: 0.8510 - val_loss: 0.4929 - val_accuracy: 0.8684\n",
            "Epoch 410/500\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.4473 - accuracy: 0.8405 - val_loss: 0.4916 - val_accuracy: 0.8421\n",
            "Epoch 411/500\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.4339 - accuracy: 0.8566 - val_loss: 0.4914 - val_accuracy: 0.8421\n",
            "Epoch 412/500\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.4656 - accuracy: 0.8590 - val_loss: 0.4916 - val_accuracy: 0.8421\n",
            "Epoch 413/500\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.4331 - accuracy: 0.8679 - val_loss: 0.4918 - val_accuracy: 0.8421\n",
            "Epoch 414/500\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.4326 - accuracy: 0.8837 - val_loss: 0.4924 - val_accuracy: 0.8421\n",
            "Epoch 415/500\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.4505 - accuracy: 0.8428 - val_loss: 0.4924 - val_accuracy: 0.8421\n",
            "Epoch 416/500\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.4650 - accuracy: 0.8409 - val_loss: 0.4913 - val_accuracy: 0.8421\n",
            "Epoch 417/500\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.4458 - accuracy: 0.8517 - val_loss: 0.4915 - val_accuracy: 0.8421\n",
            "Epoch 418/500\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.4390 - accuracy: 0.8566 - val_loss: 0.4911 - val_accuracy: 0.8421\n",
            "Epoch 419/500\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.4367 - accuracy: 0.8630 - val_loss: 0.4904 - val_accuracy: 0.8421\n",
            "Epoch 420/500\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.4474 - accuracy: 0.8613 - val_loss: 0.4899 - val_accuracy: 0.8421\n",
            "Epoch 421/500\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.4406 - accuracy: 0.8635 - val_loss: 0.4912 - val_accuracy: 0.8421\n",
            "Epoch 422/500\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.4475 - accuracy: 0.8518 - val_loss: 0.4912 - val_accuracy: 0.8421\n",
            "Epoch 423/500\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.4827 - accuracy: 0.8480 - val_loss: 0.4913 - val_accuracy: 0.8421\n",
            "Epoch 424/500\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.4456 - accuracy: 0.8717 - val_loss: 0.4908 - val_accuracy: 0.8421\n",
            "Epoch 425/500\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.4370 - accuracy: 0.8710 - val_loss: 0.4903 - val_accuracy: 0.8421\n",
            "Epoch 426/500\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.3991 - accuracy: 0.9041 - val_loss: 0.4907 - val_accuracy: 0.8421\n",
            "Epoch 427/500\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.4403 - accuracy: 0.8788 - val_loss: 0.4910 - val_accuracy: 0.8421\n",
            "Epoch 428/500\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.4355 - accuracy: 0.8781 - val_loss: 0.4907 - val_accuracy: 0.8421\n",
            "Epoch 429/500\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.4631 - accuracy: 0.8562 - val_loss: 0.4909 - val_accuracy: 0.8421\n",
            "Epoch 430/500\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.4619 - accuracy: 0.8395 - val_loss: 0.4915 - val_accuracy: 0.8421\n",
            "Epoch 431/500\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.4400 - accuracy: 0.8645 - val_loss: 0.4916 - val_accuracy: 0.8421\n",
            "Epoch 432/500\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.4398 - accuracy: 0.8630 - val_loss: 0.4914 - val_accuracy: 0.8421\n",
            "Epoch 433/500\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.4468 - accuracy: 0.8529 - val_loss: 0.4917 - val_accuracy: 0.8421\n",
            "Epoch 434/500\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.4242 - accuracy: 0.8678 - val_loss: 0.4913 - val_accuracy: 0.8421\n",
            "Epoch 435/500\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.4392 - accuracy: 0.8525 - val_loss: 0.4912 - val_accuracy: 0.8421\n",
            "Epoch 436/500\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.4336 - accuracy: 0.8593 - val_loss: 0.4919 - val_accuracy: 0.8684\n",
            "Epoch 437/500\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.4557 - accuracy: 0.8485 - val_loss: 0.4930 - val_accuracy: 0.8684\n",
            "Epoch 438/500\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.4315 - accuracy: 0.8595 - val_loss: 0.4929 - val_accuracy: 0.8684\n",
            "Epoch 439/500\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.4379 - accuracy: 0.8574 - val_loss: 0.4938 - val_accuracy: 0.8684\n",
            "Epoch 440/500\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.4775 - accuracy: 0.8392 - val_loss: 0.4922 - val_accuracy: 0.8684\n",
            "Epoch 441/500\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.4474 - accuracy: 0.8499 - val_loss: 0.4929 - val_accuracy: 0.8684\n",
            "Epoch 442/500\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.4442 - accuracy: 0.8419 - val_loss: 0.4923 - val_accuracy: 0.8421\n",
            "Epoch 443/500\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.4271 - accuracy: 0.8801 - val_loss: 0.4915 - val_accuracy: 0.8421\n",
            "Epoch 444/500\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.4796 - accuracy: 0.8314 - val_loss: 0.4904 - val_accuracy: 0.8421\n",
            "Epoch 445/500\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.4437 - accuracy: 0.8583 - val_loss: 0.4900 - val_accuracy: 0.8421\n",
            "Epoch 446/500\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.4307 - accuracy: 0.8614 - val_loss: 0.4908 - val_accuracy: 0.8421\n",
            "Epoch 447/500\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.4576 - accuracy: 0.8357 - val_loss: 0.4908 - val_accuracy: 0.8421\n",
            "Epoch 448/500\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.4660 - accuracy: 0.8396 - val_loss: 0.4910 - val_accuracy: 0.8421\n",
            "Epoch 449/500\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.4814 - accuracy: 0.8367 - val_loss: 0.4912 - val_accuracy: 0.8421\n",
            "Epoch 450/500\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.4623 - accuracy: 0.8625 - val_loss: 0.4908 - val_accuracy: 0.8421\n",
            "Epoch 451/500\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.4624 - accuracy: 0.8445 - val_loss: 0.4906 - val_accuracy: 0.8421\n",
            "Epoch 452/500\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.4635 - accuracy: 0.8576 - val_loss: 0.4897 - val_accuracy: 0.8421\n",
            "Epoch 453/500\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.4552 - accuracy: 0.8299 - val_loss: 0.4900 - val_accuracy: 0.8421\n",
            "Epoch 454/500\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.4546 - accuracy: 0.8378 - val_loss: 0.4907 - val_accuracy: 0.8421\n",
            "Epoch 455/500\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.4328 - accuracy: 0.8687 - val_loss: 0.4905 - val_accuracy: 0.8421\n",
            "Epoch 456/500\n",
            "15/15 [==============================] - 0s 6ms/step - loss: 0.4477 - accuracy: 0.8331 - val_loss: 0.4907 - val_accuracy: 0.8421\n",
            "Epoch 457/500\n",
            "15/15 [==============================] - 0s 6ms/step - loss: 0.4379 - accuracy: 0.8632 - val_loss: 0.4902 - val_accuracy: 0.8421\n",
            "Epoch 458/500\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.4904 - accuracy: 0.8381 - val_loss: 0.4908 - val_accuracy: 0.8421\n",
            "Epoch 459/500\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.4574 - accuracy: 0.8634 - val_loss: 0.4900 - val_accuracy: 0.8421\n",
            "Epoch 460/500\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.4338 - accuracy: 0.8706 - val_loss: 0.4895 - val_accuracy: 0.8421\n",
            "Epoch 461/500\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.4766 - accuracy: 0.8251 - val_loss: 0.4897 - val_accuracy: 0.8421\n",
            "Epoch 462/500\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.4705 - accuracy: 0.8330 - val_loss: 0.4907 - val_accuracy: 0.8421\n",
            "Epoch 463/500\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.4660 - accuracy: 0.8453 - val_loss: 0.4913 - val_accuracy: 0.8421\n",
            "Epoch 464/500\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.4781 - accuracy: 0.8478 - val_loss: 0.4913 - val_accuracy: 0.8421\n",
            "Epoch 465/500\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.4380 - accuracy: 0.8763 - val_loss: 0.4916 - val_accuracy: 0.8421\n",
            "Epoch 466/500\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.4213 - accuracy: 0.8575 - val_loss: 0.4918 - val_accuracy: 0.8421\n",
            "Epoch 467/500\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.4554 - accuracy: 0.8391 - val_loss: 0.4916 - val_accuracy: 0.8421\n",
            "Epoch 468/500\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.4725 - accuracy: 0.8522 - val_loss: 0.4917 - val_accuracy: 0.8421\n",
            "Epoch 469/500\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.4606 - accuracy: 0.8366 - val_loss: 0.4921 - val_accuracy: 0.8421\n",
            "Epoch 470/500\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.4693 - accuracy: 0.8598 - val_loss: 0.4918 - val_accuracy: 0.8421\n",
            "Epoch 471/500\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.4259 - accuracy: 0.8780 - val_loss: 0.4915 - val_accuracy: 0.8421\n",
            "Epoch 472/500\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.4222 - accuracy: 0.8902 - val_loss: 0.4909 - val_accuracy: 0.8421\n",
            "Epoch 473/500\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.4237 - accuracy: 0.8680 - val_loss: 0.4910 - val_accuracy: 0.8421\n",
            "Epoch 474/500\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.4375 - accuracy: 0.8769 - val_loss: 0.4916 - val_accuracy: 0.8421\n",
            "Epoch 475/500\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.4512 - accuracy: 0.8547 - val_loss: 0.4914 - val_accuracy: 0.8421\n",
            "Epoch 476/500\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.4515 - accuracy: 0.8294 - val_loss: 0.4913 - val_accuracy: 0.8421\n",
            "Epoch 477/500\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.4766 - accuracy: 0.8312 - val_loss: 0.4918 - val_accuracy: 0.8421\n",
            "Epoch 478/500\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.4119 - accuracy: 0.8796 - val_loss: 0.4923 - val_accuracy: 0.8421\n",
            "Epoch 479/500\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.4350 - accuracy: 0.8572 - val_loss: 0.4908 - val_accuracy: 0.8421\n",
            "Epoch 480/500\n",
            "15/15 [==============================] - 0s 7ms/step - loss: 0.4474 - accuracy: 0.8616 - val_loss: 0.4902 - val_accuracy: 0.8421\n",
            "Epoch 481/500\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.4433 - accuracy: 0.8581 - val_loss: 0.4898 - val_accuracy: 0.8158\n",
            "Epoch 482/500\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.4381 - accuracy: 0.8590 - val_loss: 0.4900 - val_accuracy: 0.8421\n",
            "Epoch 483/500\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.4374 - accuracy: 0.8696 - val_loss: 0.4905 - val_accuracy: 0.8421\n",
            "Epoch 484/500\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.4545 - accuracy: 0.8354 - val_loss: 0.4908 - val_accuracy: 0.8421\n",
            "Epoch 485/500\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.4689 - accuracy: 0.8328 - val_loss: 0.4920 - val_accuracy: 0.8421\n",
            "Epoch 486/500\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.4302 - accuracy: 0.8694 - val_loss: 0.4917 - val_accuracy: 0.8421\n",
            "Epoch 487/500\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.4595 - accuracy: 0.8718 - val_loss: 0.4906 - val_accuracy: 0.8421\n",
            "Epoch 488/500\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.4335 - accuracy: 0.8536 - val_loss: 0.4902 - val_accuracy: 0.8421\n",
            "Epoch 489/500\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.4621 - accuracy: 0.8199 - val_loss: 0.4917 - val_accuracy: 0.8421\n",
            "Epoch 490/500\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.4489 - accuracy: 0.8499 - val_loss: 0.4907 - val_accuracy: 0.8421\n",
            "Epoch 491/500\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.4163 - accuracy: 0.8870 - val_loss: 0.4909 - val_accuracy: 0.8421\n",
            "Epoch 492/500\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.4229 - accuracy: 0.8873 - val_loss: 0.4910 - val_accuracy: 0.8421\n",
            "Epoch 493/500\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.4338 - accuracy: 0.8581 - val_loss: 0.4914 - val_accuracy: 0.8421\n",
            "Epoch 494/500\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.4785 - accuracy: 0.8257 - val_loss: 0.4919 - val_accuracy: 0.8421\n",
            "Epoch 495/500\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.4716 - accuracy: 0.8347 - val_loss: 0.4922 - val_accuracy: 0.8421\n",
            "Epoch 496/500\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.4467 - accuracy: 0.8453 - val_loss: 0.4932 - val_accuracy: 0.8421\n",
            "Epoch 497/500\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.4289 - accuracy: 0.8721 - val_loss: 0.4926 - val_accuracy: 0.8421\n",
            "Epoch 498/500\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.4114 - accuracy: 0.8891 - val_loss: 0.4916 - val_accuracy: 0.8421\n",
            "Epoch 499/500\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.4373 - accuracy: 0.8714 - val_loss: 0.4917 - val_accuracy: 0.8421\n",
            "Epoch 500/500\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.4595 - accuracy: 0.8469 - val_loss: 0.4919 - val_accuracy: 0.8421\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Mx4N7uO-9Uy"
      },
      "source": [
        "Evaluate your testing split, to get the accuracy and the loss score."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w9Jzj9resfZF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c1cf1b50-eb6f-4626-aefe-34701738cf85"
      },
      "source": [
        "score, accuracy = model.evaluate(test_X, test_y, batch_size=16, verbose=0)\n",
        "print(\"Test fraction correct (NN-Score) = {:.2f}\".format(score))\n",
        "print(\"Test fraction correct (NN-Accuracy) = {:.2f}\".format(accuracy))\n",
        "\n",
        "test_X\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test fraction correct (NN-Score) = 0.47\n",
            "Test fraction correct (NN-Accuracy) = 0.87\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[2.50000000e-01, 0.00000000e+00, 6.66666667e-01, 1.69811321e-01,\n",
              "        3.24200913e-01, 0.00000000e+00, 0.00000000e+00, 7.70992366e-01,\n",
              "        1.00000000e+00, 0.00000000e+00, 1.00000000e+00, 0.00000000e+00,\n",
              "        6.66666667e-01, 1.03048984e-01, 0.00000000e+00, 2.96296296e-01,\n",
              "        8.01102907e-02, 9.72260158e-02, 0.00000000e+00, 0.00000000e+00,\n",
              "        5.99979555e-01, 1.00000000e+00, 0.00000000e+00, 1.00000000e+00,\n",
              "        0.00000000e+00, 2.96296296e-01],\n",
              "       [5.83333333e-01, 0.00000000e+00, 3.33333333e-01, 3.39622642e-01,\n",
              "        2.51141553e-01, 0.00000000e+00, 0.00000000e+00, 7.86259542e-01,\n",
              "        0.00000000e+00, 0.00000000e+00, 5.00000000e-01, 2.50000000e-01,\n",
              "        6.66666667e-01, 3.72107446e-01, 0.00000000e+00, 3.70370370e-02,\n",
              "        1.90589582e-01, 6.28157705e-02, 0.00000000e+00, 0.00000000e+00,\n",
              "        6.22755389e-01, 0.00000000e+00, 0.00000000e+00, 1.25000000e-01,\n",
              "        1.56250000e-02, 2.96296296e-01],\n",
              "       [6.87500000e-01, 1.00000000e+00, 3.33333333e-01, 2.45283019e-01,\n",
              "        3.53881279e-01, 0.00000000e+00, 0.00000000e+00, 2.44274809e-01,\n",
              "        0.00000000e+00, 2.25806452e-01, 5.00000000e-01, 2.50000000e-01,\n",
              "        1.00000000e+00, 4.95064145e-01, 1.00000000e+00, 3.70370370e-02,\n",
              "        1.25172817e-01, 1.13793735e-01, 0.00000000e+00, 0.00000000e+00,\n",
              "        9.31975749e-02, 0.00000000e+00, 1.15135444e-02, 1.25000000e-01,\n",
              "        1.56250000e-02, 1.00000000e+00],\n",
              "       [4.58333333e-01, 1.00000000e+00, 6.66666667e-01, 1.50943396e-01,\n",
              "        1.11872146e-01, 0.00000000e+00, 5.00000000e-01, 3.96946565e-01,\n",
              "        0.00000000e+00, 9.67741935e-02, 1.00000000e+00, 0.00000000e+00,\n",
              "        6.66666667e-01, 2.50522974e-01, 1.00000000e+00, 2.96296296e-01,\n",
              "        6.97987116e-02, 1.89339898e-02, 0.00000000e+00, 1.25000000e-01,\n",
              "        1.90621672e-01, 0.00000000e+00, 9.06313987e-04, 1.00000000e+00,\n",
              "        0.00000000e+00, 2.96296296e-01],\n",
              "       [2.08333333e-01, 0.00000000e+00, 6.66666667e-01, 4.15094340e-01,\n",
              "        2.14611872e-01, 0.00000000e+00, 5.00000000e-01, 6.18320611e-01,\n",
              "        0.00000000e+00, 0.00000000e+00, 5.00000000e-01, 0.00000000e+00,\n",
              "        6.66666667e-01, 8.08295383e-02, 0.00000000e+00, 2.96296296e-01,\n",
              "        2.50716097e-01, 4.87448864e-02, 0.00000000e+00, 1.25000000e-01,\n",
              "        4.00012455e-01, 0.00000000e+00, 0.00000000e+00, 1.25000000e-01,\n",
              "        0.00000000e+00, 2.96296296e-01],\n",
              "       [4.79166667e-01, 1.00000000e+00, 0.00000000e+00, 2.92452830e-01,\n",
              "        1.96347032e-01, 0.00000000e+00, 5.00000000e-01, 7.40458015e-01,\n",
              "        0.00000000e+00, 1.61290323e-01, 1.00000000e+00, 5.00000000e-01,\n",
              "        1.00000000e+00, 2.68935818e-01, 1.00000000e+00, 0.00000000e+00,\n",
              "        1.56573562e-01, 4.24323971e-02, 0.00000000e+00, 1.25000000e-01,\n",
              "        5.55992475e-01, 0.00000000e+00, 4.19589809e-03, 1.00000000e+00,\n",
              "        1.25000000e-01, 1.00000000e+00],\n",
              "       [3.12500000e-01, 1.00000000e+00, 6.66666667e-01, 4.33962264e-01,\n",
              "        2.48858447e-01, 0.00000000e+00, 0.00000000e+00, 8.32061069e-01,\n",
              "        0.00000000e+00, 0.00000000e+00, 1.00000000e+00, 0.00000000e+00,\n",
              "        6.66666667e-01, 1.40682273e-01, 1.00000000e+00, 2.96296296e-01,\n",
              "        2.66885894e-01, 6.18779148e-02, 0.00000000e+00, 0.00000000e+00,\n",
              "        6.94285127e-01, 0.00000000e+00, 0.00000000e+00, 1.00000000e+00,\n",
              "        0.00000000e+00, 2.96296296e-01],\n",
              "       [2.08333333e-01, 0.00000000e+00, 6.66666667e-01, 0.00000000e+00,\n",
              "        1.66666667e-01, 0.00000000e+00, 5.00000000e-01, 8.24427481e-01,\n",
              "        0.00000000e+00, 0.00000000e+00, 1.00000000e+00, 0.00000000e+00,\n",
              "        6.66666667e-01, 8.08295383e-02, 0.00000000e+00, 2.96296296e-01,\n",
              "        0.00000000e+00, 3.31456134e-02, 0.00000000e+00, 1.25000000e-01,\n",
              "        6.82025499e-01, 0.00000000e+00, 0.00000000e+00, 1.00000000e+00,\n",
              "        0.00000000e+00, 2.96296296e-01],\n",
              "       [1.25000000e-01, 1.00000000e+00, 3.33333333e-01, 2.64150943e-01,\n",
              "        1.50684932e-01, 0.00000000e+00, 5.00000000e-01, 7.86259542e-01,\n",
              "        0.00000000e+00, 0.00000000e+00, 1.00000000e+00, 0.00000000e+00,\n",
              "        6.66666667e-01, 4.27774075e-02, 1.00000000e+00, 3.70370370e-02,\n",
              "        1.37425977e-01, 2.86208958e-02, 0.00000000e+00, 1.25000000e-01,\n",
              "        6.22755389e-01, 0.00000000e+00, 0.00000000e+00, 1.00000000e+00,\n",
              "        0.00000000e+00, 2.96296296e-01],\n",
              "       [7.08333333e-01, 1.00000000e+00, 0.00000000e+00, 3.39622642e-01,\n",
              "        2.92237443e-01, 0.00000000e+00, 0.00000000e+00, 5.80152672e-01,\n",
              "        0.00000000e+00, 2.25806452e-01, 5.00000000e-01, 2.50000000e-01,\n",
              "        1.00000000e+00, 5.22182421e-01, 1.00000000e+00, 0.00000000e+00,\n",
              "        1.90589582e-01, 8.10948154e-02, 0.00000000e+00, 0.00000000e+00,\n",
              "        3.57487865e-01, 0.00000000e+00, 1.15135444e-02, 1.25000000e-01,\n",
              "        1.56250000e-02, 1.00000000e+00],\n",
              "       [3.75000000e-01, 1.00000000e+00, 0.00000000e+00, 1.50943396e-01,\n",
              "        3.40182648e-01, 0.00000000e+00, 0.00000000e+00, 3.58778626e-01,\n",
              "        1.00000000e+00, 1.61290323e-01, 5.00000000e-01, 2.50000000e-01,\n",
              "        6.66666667e-01, 1.83813729e-01, 1.00000000e+00, 0.00000000e+00,\n",
              "        6.97987116e-02, 1.05952017e-01, 0.00000000e+00, 0.00000000e+00,\n",
              "        1.62993403e-01, 1.00000000e+00, 4.19589809e-03, 1.25000000e-01,\n",
              "        1.56250000e-02, 2.96296296e-01],\n",
              "       [4.37500000e-01, 0.00000000e+00, 3.33333333e-01, 2.45283019e-01,\n",
              "        2.69406393e-01, 0.00000000e+00, 5.00000000e-01, 6.94656489e-01,\n",
              "        0.00000000e+00, 1.77419355e-01, 1.00000000e+00, 0.00000000e+00,\n",
              "        6.66666667e-01, 2.32818227e-01, 0.00000000e+00, 3.70370370e-02,\n",
              "        1.25172817e-01, 7.06087978e-02, 0.00000000e+00, 1.25000000e-01,\n",
              "        4.93832010e-01, 0.00000000e+00, 5.58474036e-03, 1.00000000e+00,\n",
              "        0.00000000e+00, 2.96296296e-01],\n",
              "       [5.83333333e-01, 0.00000000e+00, 0.00000000e+00, 3.20754717e-01,\n",
              "        4.04109589e-01, 0.00000000e+00, 0.00000000e+00, 6.71755725e-01,\n",
              "        0.00000000e+00, 0.00000000e+00, 1.00000000e+00, 2.50000000e-01,\n",
              "        6.66666667e-01, 3.72107446e-01, 0.00000000e+00, 0.00000000e+00,\n",
              "        1.76662646e-01, 1.45529378e-01, 0.00000000e+00, 0.00000000e+00,\n",
              "        4.64426329e-01, 0.00000000e+00, 0.00000000e+00, 1.00000000e+00,\n",
              "        1.56250000e-02, 2.96296296e-01],\n",
              "       [5.83333333e-01, 1.00000000e+00, 0.00000000e+00, 4.33962264e-01,\n",
              "        1.50684932e-01, 0.00000000e+00, 5.00000000e-01, 5.87786260e-01,\n",
              "        0.00000000e+00, 6.45161290e-02, 5.00000000e-01, 0.00000000e+00,\n",
              "        3.33333333e-01, 3.72107446e-01, 1.00000000e+00, 0.00000000e+00,\n",
              "        2.66885894e-01, 2.86208958e-02, 0.00000000e+00, 1.25000000e-01,\n",
              "        3.65766009e-01, 0.00000000e+00, 2.68537478e-04, 1.25000000e-01,\n",
              "        0.00000000e+00, 3.70370370e-02],\n",
              "       [4.79166667e-01, 1.00000000e+00, 6.66666667e-01, 7.35849057e-01,\n",
              "        1.66666667e-01, 1.00000000e+00, 5.00000000e-01, 6.94656489e-01,\n",
              "        0.00000000e+00, 8.06451613e-02, 1.00000000e+00, 0.00000000e+00,\n",
              "        1.00000000e+00, 2.68935818e-01, 1.00000000e+00, 2.96296296e-01,\n",
              "        5.93892724e-01, 3.31456134e-02, 1.00000000e+00, 1.25000000e-01,\n",
              "        4.93832010e-01, 0.00000000e+00, 5.24487261e-04, 1.00000000e+00,\n",
              "        0.00000000e+00, 1.00000000e+00],\n",
              "       [3.75000000e-01, 1.00000000e+00, 0.00000000e+00, 1.69811321e-01,\n",
              "        1.78082192e-01, 0.00000000e+00, 5.00000000e-01, 5.49618321e-01,\n",
              "        0.00000000e+00, 1.61290323e-02, 1.00000000e+00, 0.00000000e+00,\n",
              "        6.66666667e-01, 1.83813729e-01, 1.00000000e+00, 0.00000000e+00,\n",
              "        8.01102907e-02, 3.65787881e-02, 0.00000000e+00, 1.25000000e-01,\n",
              "        3.25486331e-01, 0.00000000e+00, 4.19589809e-06, 1.00000000e+00,\n",
              "        0.00000000e+00, 2.96296296e-01],\n",
              "       [3.54166667e-01, 1.00000000e+00, 0.00000000e+00, 2.45283019e-01,\n",
              "        2.80821918e-01, 0.00000000e+00, 0.00000000e+00, 5.57251908e-01,\n",
              "        0.00000000e+00, 1.29032258e-01, 1.00000000e+00, 0.00000000e+00,\n",
              "        1.00000000e+00, 1.68802529e-01, 1.00000000e+00, 0.00000000e+00,\n",
              "        1.25172817e-01, 7.57465394e-02, 0.00000000e+00, 0.00000000e+00,\n",
              "        3.33321580e-01, 0.00000000e+00, 2.14829982e-03, 1.00000000e+00,\n",
              "        0.00000000e+00, 1.00000000e+00],\n",
              "       [3.12500000e-01, 1.00000000e+00, 3.33333333e-01, 2.45283019e-01,\n",
              "        3.12785388e-01, 0.00000000e+00, 5.00000000e-01, 7.78625954e-01,\n",
              "        0.00000000e+00, 0.00000000e+00, 1.00000000e+00, 0.00000000e+00,\n",
              "        1.00000000e+00, 1.40682273e-01, 1.00000000e+00, 3.70370370e-02,\n",
              "        1.25172817e-01, 9.12657530e-02, 0.00000000e+00, 1.25000000e-01,\n",
              "        6.11301647e-01, 0.00000000e+00, 0.00000000e+00, 1.00000000e+00,\n",
              "        0.00000000e+00, 1.00000000e+00],\n",
              "       [6.04166667e-01, 0.00000000e+00, 0.00000000e+00, 5.66037736e-02,\n",
              "        2.78538813e-01, 0.00000000e+00, 0.00000000e+00, 3.89312977e-01,\n",
              "        0.00000000e+00, 1.61290323e-01, 5.00000000e-01, 0.00000000e+00,\n",
              "        6.66666667e-01, 3.95060443e-01, 0.00000000e+00, 0.00000000e+00,\n",
              "        2.36303766e-02, 7.47022836e-02, 0.00000000e+00, 0.00000000e+00,\n",
              "        1.84911859e-01, 0.00000000e+00, 4.19589809e-03, 1.25000000e-01,\n",
              "        0.00000000e+00, 2.96296296e-01],\n",
              "       [3.12500000e-01, 0.00000000e+00, 6.66666667e-01, 2.26415094e-01,\n",
              "        2.64840183e-01, 0.00000000e+00, 5.00000000e-01, 5.95419847e-01,\n",
              "        0.00000000e+00, 4.83870968e-02, 5.00000000e-01, 2.50000000e-01,\n",
              "        6.66666667e-01, 1.40682273e-01, 0.00000000e+00, 2.96296296e-01,\n",
              "        1.13321364e-01, 6.86117038e-02, 0.00000000e+00, 1.25000000e-01,\n",
              "        3.74156779e-01, 0.00000000e+00, 1.13289248e-04, 1.25000000e-01,\n",
              "        1.56250000e-02, 2.96296296e-01],\n",
              "       [2.29166667e-01, 1.00000000e+00, 0.00000000e+00, 1.50943396e-01,\n",
              "        9.36073059e-02, 0.00000000e+00, 0.00000000e+00, 3.28244275e-01,\n",
              "        1.00000000e+00, 3.22580645e-01, 5.00000000e-01, 0.00000000e+00,\n",
              "        1.00000000e+00, 9.16615758e-02, 1.00000000e+00, 0.00000000e+00,\n",
              "        6.97987116e-02, 1.49774555e-02, 0.00000000e+00, 0.00000000e+00,\n",
              "        1.42511691e-01, 1.00000000e+00, 3.35671847e-02, 1.25000000e-01,\n",
              "        0.00000000e+00, 1.00000000e+00],\n",
              "       [4.79166667e-01, 1.00000000e+00, 3.33333333e-01, 2.45283019e-01,\n",
              "        4.54337900e-01, 0.00000000e+00, 5.00000000e-01, 7.70992366e-01,\n",
              "        0.00000000e+00, 3.22580645e-02, 1.00000000e+00, 0.00000000e+00,\n",
              "        6.66666667e-01, 2.68935818e-01, 1.00000000e+00, 3.70370370e-02,\n",
              "        1.25172817e-01, 1.82224904e-01, 0.00000000e+00, 1.25000000e-01,\n",
              "        5.99979555e-01, 0.00000000e+00, 3.35671847e-05, 1.00000000e+00,\n",
              "        0.00000000e+00, 2.96296296e-01],\n",
              "       [4.58333333e-01, 1.00000000e+00, 0.00000000e+00, 4.33962264e-01,\n",
              "        3.08219178e-01, 0.00000000e+00, 0.00000000e+00, 8.77862595e-01,\n",
              "        1.00000000e+00, 0.00000000e+00, 1.00000000e+00, 0.00000000e+00,\n",
              "        6.66666667e-01, 2.50522974e-01, 1.00000000e+00, 0.00000000e+00,\n",
              "        2.66885894e-01, 8.89441486e-02, 0.00000000e+00, 0.00000000e+00,\n",
              "        7.70746060e-01, 1.00000000e+00, 0.00000000e+00, 1.00000000e+00,\n",
              "        0.00000000e+00, 2.96296296e-01],\n",
              "       [6.87500000e-01, 1.00000000e+00, 6.66666667e-01, 3.39622642e-01,\n",
              "        2.39726027e-01, 0.00000000e+00, 5.00000000e-01, 5.72519084e-01,\n",
              "        0.00000000e+00, 2.90322581e-01, 5.00000000e-01, 7.50000000e-01,\n",
              "        1.00000000e+00, 4.95064145e-01, 1.00000000e+00, 2.96296296e-01,\n",
              "        1.90589582e-01, 5.82056329e-02, 0.00000000e+00, 1.25000000e-01,\n",
              "        3.49321586e-01, 0.00000000e+00, 2.44704777e-02, 1.25000000e-01,\n",
              "        4.21875000e-01, 1.00000000e+00],\n",
              "       [6.25000000e-01, 1.00000000e+00, 1.00000000e+00, 3.77358491e-01,\n",
              "        1.78082192e-01, 0.00000000e+00, 5.00000000e-01, 6.94656489e-01,\n",
              "        0.00000000e+00, 1.29032258e-01, 1.00000000e+00, 5.00000000e-01,\n",
              "        6.66666667e-01, 4.18818727e-01, 1.00000000e+00, 1.00000000e+00,\n",
              "        2.19755696e-01, 3.65787881e-02, 0.00000000e+00, 1.25000000e-01,\n",
              "        4.93832010e-01, 0.00000000e+00, 2.14829982e-03, 1.00000000e+00,\n",
              "        1.25000000e-01, 2.96296296e-01],\n",
              "       [5.62500000e-01, 0.00000000e+00, 0.00000000e+00, 3.77358491e-01,\n",
              "        6.46118721e-01, 0.00000000e+00, 0.00000000e+00, 6.03053435e-01,\n",
              "        1.00000000e+00, 3.06451613e-01, 5.00000000e-01, 5.00000000e-01,\n",
              "        1.00000000e+00, 3.49945851e-01, 0.00000000e+00, 0.00000000e+00,\n",
              "        2.19755696e-01, 3.74382151e-01, 0.00000000e+00, 0.00000000e+00,\n",
              "        3.82660936e-01, 1.00000000e+00, 2.87796650e-02, 1.25000000e-01,\n",
              "        1.25000000e-01, 1.00000000e+00],\n",
              "       [6.25000000e-01, 1.00000000e+00, 1.00000000e+00, 7.16981132e-01,\n",
              "        3.69863014e-01, 0.00000000e+00, 0.00000000e+00, 6.71755725e-01,\n",
              "        0.00000000e+00, 3.22580645e-02, 5.00000000e-01, 0.00000000e+00,\n",
              "        1.00000000e+00, 4.18818727e-01, 1.00000000e+00, 1.00000000e+00,\n",
              "        5.69420996e-01, 1.23375335e-01, 0.00000000e+00, 0.00000000e+00,\n",
              "        4.64426329e-01, 0.00000000e+00, 3.35671847e-05, 1.25000000e-01,\n",
              "        0.00000000e+00, 1.00000000e+00],\n",
              "       [8.54166667e-01, 1.00000000e+00, 3.33333333e-01, 5.84905660e-01,\n",
              "        2.71689498e-01, 0.00000000e+00, 0.00000000e+00, 5.49618321e-01,\n",
              "        0.00000000e+00, 0.00000000e+00, 1.00000000e+00, 0.00000000e+00,\n",
              "        6.66666667e-01, 7.37279703e-01, 1.00000000e+00, 3.70370370e-02,\n",
              "        4.13678325e-01, 7.16197063e-02, 0.00000000e+00, 0.00000000e+00,\n",
              "        3.25486331e-01, 0.00000000e+00, 0.00000000e+00, 1.00000000e+00,\n",
              "        0.00000000e+00, 2.96296296e-01],\n",
              "       [7.70833333e-01, 0.00000000e+00, 6.66666667e-01, 4.90566038e-01,\n",
              "        3.47031963e-01, 0.00000000e+00, 0.00000000e+00, 6.18320611e-01,\n",
              "        0.00000000e+00, 0.00000000e+00, 5.00000000e-01, 2.50000000e-01,\n",
              "        6.66666667e-01, 6.08841034e-01, 0.00000000e+00, 2.96296296e-01,\n",
              "        3.18234010e-01, 1.09830567e-01, 0.00000000e+00, 0.00000000e+00,\n",
              "        4.00012455e-01, 0.00000000e+00, 0.00000000e+00, 1.25000000e-01,\n",
              "        1.56250000e-02, 2.96296296e-01],\n",
              "       [4.16666667e-01, 0.00000000e+00, 0.00000000e+00, 3.39622642e-01,\n",
              "        3.26484018e-01, 0.00000000e+00, 5.00000000e-01, 7.02290076e-01,\n",
              "        0.00000000e+00, 0.00000000e+00, 1.00000000e+00, 0.00000000e+00,\n",
              "        6.66666667e-01, 2.15807694e-01, 0.00000000e+00, 0.00000000e+00,\n",
              "        1.90589582e-01, 9.84451250e-02, 0.00000000e+00, 1.25000000e-01,\n",
              "        5.03879448e-01, 0.00000000e+00, 0.00000000e+00, 1.00000000e+00,\n",
              "        0.00000000e+00, 2.96296296e-01],\n",
              "       [6.45833333e-01, 1.00000000e+00, 6.66666667e-01, 4.33962264e-01,\n",
              "        1.34703196e-01, 0.00000000e+00, 0.00000000e+00, 6.41221374e-01,\n",
              "        0.00000000e+00, 4.83870968e-01, 5.00000000e-01, 0.00000000e+00,\n",
              "        6.66666667e-01, 4.43396183e-01, 1.00000000e+00, 2.96296296e-01,\n",
              "        2.66885894e-01, 2.44143640e-02, 0.00000000e+00, 0.00000000e+00,\n",
              "        4.26909161e-01, 0.00000000e+00, 1.13289248e-01, 1.25000000e-01,\n",
              "        0.00000000e+00, 2.96296296e-01],\n",
              "       [6.45833333e-01, 1.00000000e+00, 0.00000000e+00, 2.92452830e-01,\n",
              "        3.01369863e-01, 0.00000000e+00, 0.00000000e+00, 5.34351145e-01,\n",
              "        1.00000000e+00, 4.51612903e-01, 5.00000000e-01, 2.50000000e-01,\n",
              "        1.00000000e+00, 4.43396183e-01, 1.00000000e+00, 0.00000000e+00,\n",
              "        1.56573562e-01, 8.55278618e-02, 0.00000000e+00, 0.00000000e+00,\n",
              "        3.10141535e-01, 1.00000000e+00, 9.21083549e-02, 1.25000000e-01,\n",
              "        1.56250000e-02, 1.00000000e+00],\n",
              "       [6.04166667e-01, 1.00000000e+00, 6.66666667e-01, 4.33962264e-01,\n",
              "        1.94063927e-01, 1.00000000e+00, 0.00000000e+00, 7.17557252e-01,\n",
              "        0.00000000e+00, 0.00000000e+00, 1.00000000e+00, 0.00000000e+00,\n",
              "        6.66666667e-01, 3.95060443e-01, 1.00000000e+00, 2.96296296e-01,\n",
              "        2.66885894e-01, 4.16759561e-02, 1.00000000e+00, 0.00000000e+00,\n",
              "        5.24347209e-01, 0.00000000e+00, 0.00000000e+00, 1.00000000e+00,\n",
              "        0.00000000e+00, 2.96296296e-01],\n",
              "       [5.83333333e-01, 1.00000000e+00, 6.66666667e-01, 5.28301887e-01,\n",
              "        0.00000000e+00, 1.00000000e+00, 5.00000000e-01, 7.78625954e-01,\n",
              "        0.00000000e+00, 3.22580645e-02, 1.00000000e+00, 2.50000000e-01,\n",
              "        1.00000000e+00, 3.72107446e-01, 1.00000000e+00, 2.96296296e-01,\n",
              "        3.54898642e-01, 0.00000000e+00, 1.00000000e+00, 1.25000000e-01,\n",
              "        6.11301647e-01, 0.00000000e+00, 3.35671847e-05, 1.00000000e+00,\n",
              "        1.56250000e-02, 1.00000000e+00],\n",
              "       [3.33333333e-01, 1.00000000e+00, 0.00000000e+00, 4.52830189e-01,\n",
              "        4.17808219e-01, 0.00000000e+00, 0.00000000e+00, 5.80152672e-01,\n",
              "        1.00000000e+00, 0.00000000e+00, 5.00000000e-01, 7.50000000e-01,\n",
              "        1.00000000e+00, 1.54430005e-01, 1.00000000e+00, 0.00000000e+00,\n",
              "        2.83524348e-01, 1.55030207e-01, 0.00000000e+00, 0.00000000e+00,\n",
              "        3.57487865e-01, 1.00000000e+00, 0.00000000e+00, 1.25000000e-01,\n",
              "        4.21875000e-01, 1.00000000e+00],\n",
              "       [5.62500000e-01, 1.00000000e+00, 0.00000000e+00, 2.92452830e-01,\n",
              "        2.80821918e-01, 1.00000000e+00, 0.00000000e+00, 5.57251908e-01,\n",
              "        1.00000000e+00, 1.93548387e-01, 5.00000000e-01, 2.50000000e-01,\n",
              "        6.66666667e-01, 3.49945851e-01, 1.00000000e+00, 0.00000000e+00,\n",
              "        1.56573562e-01, 7.57465394e-02, 1.00000000e+00, 0.00000000e+00,\n",
              "        3.33321580e-01, 1.00000000e+00, 7.25051190e-03, 1.25000000e-01,\n",
              "        1.56250000e-02, 2.96296296e-01],\n",
              "       [1.25000000e-01, 1.00000000e+00, 0.00000000e+00, 2.45283019e-01,\n",
              "        1.64383562e-01, 0.00000000e+00, 5.00000000e-01, 4.50381679e-01,\n",
              "        1.00000000e+00, 2.58064516e-01, 5.00000000e-01, 0.00000000e+00,\n",
              "        1.00000000e+00, 4.27774075e-02, 1.00000000e+00, 0.00000000e+00,\n",
              "        1.25172817e-01, 3.24793047e-02, 0.00000000e+00, 1.25000000e-01,\n",
              "        2.33253814e-01, 1.00000000e+00, 1.71863986e-02, 1.25000000e-01,\n",
              "        0.00000000e+00, 1.00000000e+00],\n",
              "       [7.29166667e-01, 1.00000000e+00, 1.00000000e+00, 7.16981132e-01,\n",
              "        2.30593607e-01, 0.00000000e+00, 0.00000000e+00, 6.41221374e-01,\n",
              "        0.00000000e+00, 9.67741935e-02, 5.00000000e-01, 0.00000000e+00,\n",
              "        1.00000000e+00, 5.50175404e-01, 1.00000000e+00, 1.00000000e+00,\n",
              "        5.69420996e-01, 5.46583525e-02, 0.00000000e+00, 0.00000000e+00,\n",
              "        4.26909161e-01, 0.00000000e+00, 9.06313987e-04, 1.25000000e-01,\n",
              "        0.00000000e+00, 1.00000000e+00]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6YaAr-aeAHBW"
      },
      "source": [
        "### 1.2.2.6 Visualize Accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r-h-R6YS_Ptd"
      },
      "source": [
        "Plot the training and validation accuracy. Try to interpret those plots. \n",
        "\n",
        "\n",
        "**TODO: for each experiment you make, include this plot and indicate whether there exist any type of overfitting or underfitting.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G0eaLehNp0ES",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "outputId": "a1b59d81-3b09-40fa-c584-275b6a976d08"
      },
      "source": [
        "# Get training and test loss histories\n",
        "training_loss = hist.history['accuracy']\n",
        "val_loss = hist.history['val_accuracy']\n",
        "\n",
        "# Create count of the number of epochs\n",
        "epoch_count = range(1, len(training_loss) + 1)\n",
        "\n",
        "# Visualize loss history\n",
        "plt.figure()\n",
        "plt.plot(epoch_count, training_loss, 'r--')\n",
        "plt.plot(epoch_count, val_loss, 'b-')\n",
        "plt.legend(['Training Accuracy', 'Validation Accuracy'])\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z0vr8s80Aljv"
      },
      "source": [
        "## 1.2.3 Conclusion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wK6q-eIkAqmG"
      },
      "source": [
        "That's it! Congratulations on training a logistic regression model.\n",
        "\n",
        "Make sure you deliver all the requirements for the submission."
      ]
    }
  ]
}
